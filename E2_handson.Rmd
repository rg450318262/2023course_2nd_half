---
title: "E2 hands-on"
output: 
   html_document:
     code_folding: hide
date: "2023-11-18"
---



```{r}
install.packages("ISLR2")
library(ISLR2)
names(Hitters)
Hitters <- na.omit(Hitters)
sum(is.na(Hitters))
```

```{r}
install.packages("leaps")
library(leaps)
regfit.full <- regsubsets(Salary ~ ., Hitters)
regfit.full |> names()
regfit.full |> summary()
```

```{r}
regfit.full <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
regfit.full |> summary()
```


```{r}
reg.summary <- summary(regfit.full)
reg.summary$rsq
```

```{r}
reg.summary$rsq
```



```{r}
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables",
ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables",
ylab = "Adjusted RSq", type = "l")
points(11, reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
```

```{r}
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2") 
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")

```

```{r}
coef(regfit.full, 6)
```

```{r}
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters,nvmax = 19, method = "backward") 
summary(regfit.bwd)
```


```{r}
```


```{r}
```


```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE) 
test <- (!train)
```


```{r}
regfit.best <- regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19)
```


```{r}
test.mat <- model.matrix(Salary ~., data = Hitters[test, ])
```


```{r}
val.errors <- rep(NA, 19) 
for (i in 1:19) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi 
val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
val.errors
```





```{r}
options(digits=4)
min <- which.min(val.errors)
coef(regfit.best, min)
```


```{r}
predict.regsubsets <-  function(object, newdata, id, ...) { 
    form <- as.formula(object$call[[2]])
    mat <- model.matrix(form, newdata)
    coefi <- coef(object, id = id)
    xvars <- names(coefi) 
    mat[, xvars] %*% coefi
}
```
```{r}
regfit.best <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
```




```{r}
k <- 10
n <- nrow(Hitters)
set.seed(1)
folds <- sample(rep(1:k, length = n)) 
cv.errors <- matrix(NA, k, 19, dimnames = list(NULL, c(1:19)))
cv.errors
```

```{r}
for (j in 1:k) {
best.fit <- regsubsets(Salary ~ .,data = Hitters[folds != j, ],nvmax = 19) 

for (i in 1:19) {
pred <- predict(best.fit, Hitters[folds == j, ], id = i) 
cv.errors[j, i] <- mean((Hitters$Salary[folds == j] - pred)^2) 
}
}

mean.cv.errors <- apply(cv.errors, 2, mean)
which.min(mean.cv.errors)

```


```{r}
par(mfrow = c(1, 1))
plot(mean.cv.errors, type = "b")
```


```{r}
reg.best <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
coef(reg.best, 10)
```


```{r}
x <- model.matrix(Salary ~ ., Hitters)[, -1] 
y <- Hitters$Salary
```

```{r}
library(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
```
```{r}
coef(ridge.mod)
```

```{r}
ridge.mod$lambda[50]
```
```{r}
coef(ridge.mod)[, 50]
```

```{r}
sqrt(sum(coef(ridge.mod)[-1, 50]^2))
```

```{r}
coef(ridge.mod)[, 60]
sqrt(sum(coef(ridge.mod)[-1, 60]^2))
```

```{r}
predict(ridge.mod, s = 50, type ="coefficients")[1:20, ]
```

```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2) 
test <- (-train)
y.test <- y[test]
```


```{r}
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
coef(ridge.mod)[-1,]
```

```{r}
ridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ], exact = T, x = x[train, ], y = y[train])
mean((ridge.pred - y.test)^2)
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = T, type = "coefficients",x = x[train, ], y = y[train])[1:20, ]
```


```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0) 
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```


```{r}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])
mean((ridge.pred - y.test)^2)
```


```{r}
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```


```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1) 
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])
mean((lasso.pred - y.test)^2) 
```


```{r}
```


```{r}
set.seed(123)  # For reproducibility

# Create synthetic data
n <- 100  # number of observations
p <- 5    # number of predictors
X <- matrix(rnorm(n * p), n, p)
beta <- rnorm(p)
intercept <- 2
probabilities <- 1 / (1 + exp(-(X %*% beta + intercept)))
y <- rbinom(n, 1, probabilities)
# Create a data frame
data <- as.data.frame(cbind(X, y))
```


```{r}
library(glmnet)

# Prepare data for glmnet
x <- as.matrix(data[, -which(names(data) == "y")])
y_glmnet <- data$y

# Set a very small lambda value
lambda_ols <- 1e-10

# Fit the model using glmnet
model_glmnet <- glmnet(x, y_glmnet, family = "binomial", alpha = 0, lambda = lambda_ols)

# Fit the model using glm
model_glm <- glm(y ~ ., data = data, family = "binomial")

# View coefficients
coef(model_glmnet, s = lambda_ols)  # Coefficients from glmnet
coef(model_glm)   
```





```{r eval = F}
# discarded
train.set.x <- 
  train.set[,which(colnames(train.set) != "SPAM")]

train.set.y <- 
  train.set$SPAM 

lambda_ols <- 1e-10

grid <- 10^seq(10, -2, length = 100)

fit.regular <- 
  glmnet(
    train.set.x, 
    train.set.y, 
    alpha = 0, 
    lambda = 0,  
    family = "binomial"
    )

coef.lm <-  
  predict(
    fit.regular, 
    s = 0, 
    type = "coefficients", 
    exact = T,  
    x= train.set.x,
    y = train.set.y
    )
coef.lm
#coef(fit.regular, s= lambda_ols)
```
```{r}
```

```{r}
# Previous get_accuracy and get_perplexity function does not work for the results from glmnet, I write a
# modified version here
# Function to derive accuracy
derive_accuracy <- function(predictions, true_values) {
  correct <- sum((predictions > 0.5) == true_values)
  accuracy <- correct / length(true_values)
  return(accuracy)
}

# Function to derive perplexity
derive_perplexity <- function(prob_test, y_test) {
  #n_test <- length(y_test)
  #likelihood <- ifelse(y_test == 1, prob_test, 1 - prob_test)
  log_likelihood_test <- 
    mean((y_test*log(prob_test) + (1-y_test)*log(1-prob_test)))
  ## perplexity for test
  perplexity_test<- exp(-log_likelihood_test)
  return(perplexity)
}
```

```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


