---
title: "E2 assignment"
output: 
   html_document:
     code_folding: hide
date: "2023-11-18"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, warning = T, message = T)
```

# Problem 8

Performance measures In this exercise set, you will train probabilistic
classifiers which estimate $\hat{p}(y | x)$): the probability of class ùë¶
given the covariate vector x. Two commonly used performance measures for
probabilistic classifiers are accuracy and perplexity. We use the
following notation:

‚Ä¢ $y_i \in \{0,1\}$: the true class of point ùëñ.

‚Ä¢ $\hat{p} = \hat{p}(y = 1 | x)$: the estimated probability for the ùëñth
point in a dataset of size ùëõ being spam.

‚Ä¢ $\hat{y}$: the predicted class for point i which is $\hat{y} = 1$ if
$\hat{p} \geq 0.5$ and $\hat{y} = 0$.We define the accuracy on a dataset
of n items as follows:

$$
accuracy = \Sigma_{i=1}^{n}I(y = \hat{y_i})/n
$$

$$
perplexity = \exp(-\Sigma_{i=1}^{n}log\hat{p}(y = y_i|x_i)/n)
$$

Perplexity is a transformation of the likelihood (perplexity =
exp(‚àíloglikelihood/ùëõ)), which may be the most commonly used performance
measure on probabilistic classifiers. Example values are perplexity = 1
for a perfect classifier, which always predicts the probability of one
to an actual class, and perplexity = 2 for coin flipping, which has a
predicted class probabilit $\hat{p} = 1/2$.

## Problem 8 Task a

### Question

Using one-hot encoding for $y_i$, train a logistic regression model
without Lasso or Ridge regularisation on the training data. Then: (i)
Report the model coeÔ¨Äicients. (ii) Compute and report the accuracy and
perplexity on the training and testing data. Make sure that you use no
regularisation. Notice that you may get warnings about convergence; why?
(iii) Write down the equation for the predicted class probability, given
the model coeÔ¨Äicients $\beta$ and the covariate vector $x$.

### My answer

```{r}

library(tidyverse)
library(here)

#read data
train.set <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "spam_train.csv"))

test.set <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "spam_test.csv"))
```

**The table below shows the coefficients for the model without
regularization.**

```{r}
train.set |> map(is.na) |> map(sum)
test.set |> map(is.na) |> map(sum)
```

```{r waning = T, message=T}
#load packages
library(broom)
library(kableExtra)
library(glmnet)
#train model on train set
fit <- glm(SPAM ~ ., data = train.set, family = binomial(link ="logit"))

#check output (model coefficients)
coef(fit) |> 
  tidy() |> 
  rename(
    variable = names,
    coefficient = x) |>
  mutate_if(is.numeric, ~round(.x, 3)) |> 
  DT::datatable()
#caption = "Fig P1_Task_a_1. Model coefficient on train set"

```

**The table below shows the accuracy on training and testing set.**

```{r}
# function to derive accuracy

get_accuracy <- 
  function(fit.train, test.set, true.y){
    
    # Train accuracy 
  
    ##get sample size
    n_train <-  nrow(fit.train$model[1]) #1 is list of true y 
    ##get true y
    y_train <- fit.train$model[1]
    ##get predicted probability
    prob_train <- fit.train$fitted.values
    ## assign probability to class(1/0)
    class_train <- ifelse(prob_train>=0.5, 1, 0)
    ## indicator function (0/1) for test
    indicator_train <- ifelse(class_train == y_train, 1, 0)
    ##accuracy
    accuracy_train = sum(indicator_train/n_train)
    
    # Test accuracy
    
    ## get sample size for test set
    n_test <- nrow(test.set)
    ## get true y for test set
    y_test <- test.set[,true.y]
    ## get predicted probability 
    prob_test <- predict(fit.train, test.set, type = "response")
    ## assign probability to class(1/0)
    class_test <- if_else(prob_test>=0.5, 1, 0)
    ## indicator function (0/1) for test
    indicator_test <- if_else(class_test == y_test, 1, 0)
    ## accuracy for test
    accuracy_test <- sum(indicator_test/n_test)
    
    ##print results
    output <- data.frame(accuracy_train = accuracy_train, accuracy_test =accuracy_test)
    print(output)
  }

get_accuracy(fit, test.set, "SPAM") |> 
  DT::datatable()
baseline_test_accuracy<- get_accuracy(fit, test.set, "SPAM")[1,2]
```

**The table below shows the perplexity on training and testing set.**

```{r}
# function to derive perplexity

get_perplexity <-  #true.y is the variable name of y in test.set
  function(fit.train, test.set, true.y){
    
    # Train perplexity 
  
    ##get sample size
    n_train <-  nrow(fit.train$model[1]) #1 is list of true y 
    ##get true y
    y_train <- fit.train$model[1]
    ##get predicted probability
    prob_train <- fit.train$fitted.values
    ##Indicator function (0/1)
    indicator_train <- if_else(prob_train>=0.5, 1, 0)
    ##log likelihood for train
    log_likelihood_train <- sum(y_train*log(prob_train) + (1-y_train)*log(1-prob_train))
    ##perplexity for train
    perplexity_train <- exp(-log_likelihood_train/n_train)
    
    
    # Test perplexity

    ## get sample size for test set
    n_test <- nrow(test.set)
    ## get true y for test set
    y_test <- test.set[, true.y] 
    ## get predicted probability 
    prob_test <- predict(fit.train, test.set, type = "response")
    ## og likelihood for test
     log_likelihood_test <- sum(y_test*log(prob_test) + (1-y_test)*log(1-prob_test))
    ## perplexity for test
    perplexity_test<- exp(-log_likelihood_test/n_test)
    
    ##print results
    output <- data.frame(perplexity_train = perplexity_train, perplexity_test =perplexity_test)
    print(output)
  }

# generate the table 
p1.task.a.table.1 <- get_perplexity(fit, test.set, "SPAM") |>
  mutate_if(is.numeric, ~round(.x,3))
p1.task.a.table.1|> 
  DT::datatable()

#get values to report
baseline_test_perplexity <- get_perplexity(fit, test.set, "SPAM")[1,2]

```

**Why don't converge?** The warning of convergence can be the result of
overly complex model, multi-colinearity and too few observations in
terms of predictors. In our case, we have fairly small number of
predictors against observations. I suppose the problem is mostly due to
multi-colinearity. This can be further consolidated by the common fact
that a spam email usually has more than one of these identifying
features. To tell the truth, I say it mostly because task b is about
regularization. Couldn't find a safer guess. :)

**Below is the equation for the predicted class probability, given the
model coeÔ¨Äicients** $\beta$ and the covariate vector $x$.

$$
\hat{p}(y = 1 \mid x) = \frac{1}{1 + e^{-(-55.0 + 17.5 \times x_1 + 0 \times x_2+ 17.9 \times x_3 + 34.9 \times x_4 + 38.1 \times x_5)}} \\
$$ Where $x_1$ is MISSING_FROM; $x_2$ is FROM_ADDR_WS; $x_3$ is
TVD_SPACE_RATIO; $x_4$ is LOTS_OF_MONEY; $x_5$ is
T_FILL_THIS_FORM_SHORT.

## Problem 8 Task b

### Question

Train a logistic regression model with Lasso regularisation. Find a
regularisation coeÔ¨Äicient value that performs better than the
unregularised version on the test data and has some regression
coeÔ¨Äicients equal to zero. You can do this by trying various values;
there is no need to be more sophisticated here. Report your parameters,
the regression coeÔ¨Äicients, and the accuracies and perplexities on the
testing data. Look at the predicted class probabilities for the
unregularized regressor in Task a, and your regularized regressors in
Task b. How do the distributions of the probabilities differ?

### My answer

**I trained LASSO models with a series of lambda values using train
dataset.** Then I predicted the probability of SPAM by trying different
lambda values including $\lambda$ = 0.0005, 0.05, 0.5 and 5. I selected
these $\lambda$ values arbitrarily. The table below shows their
parameters, accuracy (test set) and perplexity(test set). The first row
also exhibits un-regularized model created in task a, for the purpose of
comparing. I noticed that the lambda = 0.0005 model and un-regularized
model have same level of testing accuracy, but the former outperforms in
terms of much smaller perplexity value. Actually, the un-regularized
model's perplexity indicates it is worse than random guess. So to answer
the next part of question, I will visualize their probabilities

```{r}
# extract train set x frame
train.set.x <- 
  train.set[,which(colnames(train.set) != "SPAM")] |> 
  as.matrix()
# extract train set y vedtor
train.set.y <- 
  train.set$SPAM 

# extract test set x frame
test.set.x <- 
  test.set[,which(colnames(train.set) != "SPAM")] |> 
  as.matrix()
# extract est set y vedtor
test.set.y <- 
  test.set$SPAM 

#define lambada values to test out
grid <- 10^seq(10, -2, length = 100)

#fit a model with lasso regularization
fit.regular <- 
  glmnet(
    train.set.x, 
    train.set.y, 
    alpha = 1, #lasso
    lambda = grid,  
    family = "binomial"
    )
```

```{r}
#try various lambda values

## try lambda = 0
prediction.l0 <-  
  predict(
    fit.regular, 
    s = 0, 
    newx = test.set.x,
    type = "response"
    )

## try lambda = 0.0005
prediction.l00005 <-  
  predict(
    fit.regular, 
    s = 0.0005, 
    newx = test.set.x,
    type = "response"
    )

## try lambda = 0.05
prediction.l005 <-  
  predict(
    fit.regular, 
    s = 0.05, 
    newx = test.set.x,
    type = "response"
    )
## try lambda = 0.5
prediction.l05<-  
  predict(
    fit.regular, 
    s = 0.5, 
    newx = test.set.x,
    type = "response"
  )
## try lambda = 5
prediction.l5<-  
  predict(
    fit.regular, 
    s = 5, 
    newx= test.set.x,
    type = "response"
    )

prd.lm.s005 <-  
  predict(
    fit.regular, 
    s = 0.000026, 
    newx= test.set.x,
    type = "response"
    )
```

```{r}
# obtain accuracy and perplexity for the models created

#lambda = 0
## accuracy
predicted_classes <- ifelse(prediction.l0 > 0.5, 1, 0)
accuracy_test_l0 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l0 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l0), log(1 - prediction.l0))))
## Coefficient
coef.l0 <-  
  predict(
    fit.regular, 
    s = 0, 
    newx = test.set.x,
    type = "coefficient"
    )
#lambda = 0.0005
## accuracy
predicted_classes <- ifelse(prediction.l00005 > 0.5, 1, 0)
accuracy_test_l00005 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l00005 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l00005), log(1 - prediction.l00005))))
## Coefficient
coef.l00005 <-  
  predict(
    fit.regular, 
    s = 0.0005, 
    newx = test.set.x,
    type = "coefficient"
    )

#lambda = 0.05
##accuracy
predicted_classes <- ifelse(prediction.l005 > 0.5, 1, 0)
accuracy_test_l005 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l005 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l005), log(1 - prediction.l005))))
## Coefficient
coef.l005 <-  
  predict(
    fit.regular, 
    s = 0.05, 
    newx = test.set.x,
    type = "coefficient"
    )
#lambda = 0.5
##accuracy
predicted_classes <- ifelse(prediction.l05 > 0.5, 1, 0)
accuracy_test_l05 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l05 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l05), log(1 - prediction.l05))))
## Coefficient
coef.l05 <-  
  predict(
    fit.regular, 
    s = 0.5, 
    newx = test.set.x,
    type = "coefficient"
    )

#lambda = 5
## accuracy
predicted_classes<- ifelse(prediction.l5 > 0.5, 1, 0)
accuracy_test_l5 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l5 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l5), log(1 - prediction.l5))))
## Coefficient
coef.l5 <-  
  predict(
    fit.regular, 
    s = 5, 
    newx = test.set.x,
    type = "coefficient"
    )

coefficient.l00005 <-  
  predict(
    fit.regular, 
    s = 0.0005, 
    newx = test.set.x,
    type = "coefficient"
    )
```

```{r}
p1_taskb_table <- 
  tibble(
    model = c(
      "no regularization",
      #"lambda = 0",
      "lambda = 0.0005",
      "lambda = 0.05",
      "lambda = 0.5",
      "lambda = 5"),
    `accuracy(test set)` = c(
      baseline_test_accuracy, 
      #accuracy_test_l0, 
      accuracy_test_l00005,
      accuracy_test_l005,
      accuracy_test_l05,
      accuracy_test_l5
      ), 
    `perplexity(test set)` = c(
      baseline_test_perplexity, 
      #perplexity_test_l0, 
      perplexity_test_l00005,
      perplexity_test_l005,
      perplexity_test_l05,
      perplexity_test_l5
      )
  )

p1_taskb_table



#coef.l0.parameters <- coef.l0 |> as.matrix() |> data.frame() 
coef.l00005.parameters <- coef.l00005 |> as.matrix() |> data.frame() 
coef.l005.parameters <- coef.l005 |> as.matrix() |> data.frame() 
coef.l05.parameters <- coef.l05 |> as.matrix() |> data.frame() 
coef.l5.parameters <- coef.l5 |> as.matrix() |> data.frame() 
baseline <- fit$coefficients |> data.frame()
parameters <- 
  cbind(
    baseline,
    #coef.l0.parameters,
    coef.l00005.parameters,
    coef.l005.parameters, 
    coef.l05.parameters,
    coef.l5.parameters
    ) 

colnames(parameters) <- c(
  #"l0", 
  "l00005", "l005", "l05", "l5")
parameter.table <- parameters |> t() |> data.frame()
rownames(parameter.table) <- NULL
p1_taskb_table <- cbind(p1_taskb_table, parameter.table)
```

```{r}
p1_taskb_table |> 
  mutate_if(is.numeric, ~round(.x,3)) |> 
  DT::datatable()
```

**To analyze how the distributions of probabilities differ between the
regularized and unregularized models, I visualized and overlaid their
density plots.** On each side of p\>0.5 ($\hat{y} = 1$) and
p\<0.5$\hat{y} = 0$, the LASSO model with $\lambda = 0.0005$ shows more
centered and higher peaks.

```{r}
plot(density(prediction.l00005), col = "blue", cex.main = 0.9, main = "Distribution of probability non-regularized model vs Lasso model (lambda = 0.0005)", xlab = "Predicted probability")
lines(density(fit$fitted.values), col = "red")
abline(v = 0.5, col = "black")
text(0.7, 2, "<-Classifiction cut off")
text(-0.15, 2.7, "predicted_y = 0", cex = 0.8)
text(0.65, 2.7, "predicted_y = 1", cex = 0.8)

legend("topright", c("Non-regularized model", "Lasso model"), 
       col =c("blue","red"), cex =0.7, lty=1)
```

# Problem 9

[6 points] Objective: generative Bayes classifier In this problem, you
will study the quadratic discriminant analysis (QDA) classifier.
Consider a simple case with two classes and only one feature ($K$ = 2
and $p$ = 1).

## Question Task a

Prove that the QDA classifier is not linear if the class-specific
variances differ (ùúé12 ‚â† ùúé2). Hint: This problem is from the textbook
(Problem 3, page 189). Please see the discussion in the textbook for
hints and guidance. For this problem, you should follow the arguments
laid out in Sect. 4.4.1 of the textbook, but without assuming that ùúé12 =
ùúé2.

## My answer

In QDA, the decision boundary between two classes is determined by the
point where their respective probability distributions are equal. For a
single feature $x$ and two classes, the class-conditional densities for
a normal distribution are given by:

$$ 
p(x | y = k) = \frac{1}{\sqrt{2\pi\sigma_k^2}} \exp\left(-\frac{(x - \mu_k)^2}{2\sigma_k^2}\right) 
$$

where \$ k \$ is the class index (1 or 2), \$ \mu\_k \$ is the mean of
class \$ k \$, and \$ \sigma\_k\^2 \$ is the variance of class \$ k \$.

\\section\*{Decision Rule:}

The QDA classifier assigns a point \$ x \$ to the class for which \$ p(x
\| y = k)\$ is higher. The decision boundary is where
$$ p(x | y = 1) = p(x | y = 2) $$

Setting the class-conditional densities equal to each other, we get:

$$ \frac{1}{\sqrt{2\pi\sigma_1^2}} \exp\left(-\frac{(x - \mu_1)^2}{2\sigma_1^2}\right) = \frac{1}{\sqrt{2\pi\sigma_2^2}} \exp\left(-\frac{(x - \mu_2)^2}{2\sigma_2^2}\right) $$

Taking the logarithm of both sides simplifies this to:

$$ -\frac{(x - \mu_1)^2}{2\sigma_1^2} + \log(\sigma_2) = -\frac{(x - \mu_2)^2}{2\sigma_2^2} + \log(\sigma_1) $$

Expanding and rearranging terms, we get a quadratic equation in \$ x \$:

$$ a x^2 + b x + c = 0 $$

where \$ a \$, \$ b \$, and \$ c \$ are coefficients that depend on \$
\mu\_1 \$, \$\mu\_2 \$, \$\sigma\_1\^2 \$, and \$ \sigma\_2\^2 \$.

If \$ \sigma\_1\^2 = \sigma\_2\^2 \$, the \$ x\^2 \$ terms cancel out,
and the decision boundary becomes linear. However, if \$ \sigma\_1\^2
\neq \sigma\_2\^2 \$, the \$ x\^2 \$ term remains, making the decision
boundary quadratic, hence non-linear.

# Problem 10

Objective: naive Bayes classifier

```{r}
library(magick)
palmer <- image_read("https://const-ae.name/img/penguins_vs.png") |>
  image_convert("svg")
```

`r palmer`

In this problem, you will study the Palmer penguins by building your own
Naive Bayes classifier.

Artwork by @allison_horst

‚Ä¢ Dataset description.

‚Ä¢ Use penguins_train.csv as your training data and penguins_test.csv as
your testing data.

‚Ä¢ Binary classification task: classify the penguin species as $y \in$
{Adelie, notAdelie} (not Adelie combining Gentoo and Chinstrap species)
based on four morphological and weight measurements of the individual
penguins, denoted by $x=(x_1, x_2, x_3, x_4)^T \in \mathbb{R}_4$

Naive Bayes (NB) classifier

Your task is to build your own NB classifier; you should not use a
ready-made classifier from a library. However, you do not need to create
a generic classifier (such as naive Bayes in the R e1071 library); it is
enough that your classifier works for this particular task.

The idea of NB is that the dimensions are conditionally independent,
given the class. Each class conditional feature distribution $p(x_i|y)$
is assumed to originate from an independent Gaussian distribution with
its mean $\mu$ and variance $\sigma^2$ for $i = 1,2,3,4$

## Problem 10 Task a

### Question

Compute and report each attribute's means and standard deviations
separately in the training set for both classes.

Estimate and report the class probabilities using Laplace smoothing with
a pseudocount of 1 on the training set. (You should produce a total of
18 numbers from this task.)

```{r}
# read train set
train <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "penguins_train.csv"))
# read test set
test <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "penguins_test.csv"))
```

```{r}
bill_length_mean_Adelie <- mean(train |> filter(species == "Adelie") |> select(bill_length_mm) |> pull())
bill_length_sd_Adelie <- sd(train |> filter(species == "Adelie") |> select(bill_length_mm) |> pull())
bill_depth_mean_Adelie <- mean(train |> filter(species == "Adelie") |> select(bill_depth_mm) |> pull())
bill_depth_sd_Adelie <- sd(train |> filter(species == "Adelie") |> select(bill_depth_mm) |> pull())
flipper_length_mean_Adelie <- mean(train |> filter(species == "Adelie") |> select(flipper_length_mm) |> pull())
flipper_length_sd_Adelie <- sd(train |> filter(species == "Adelie") |> select(flipper_length_mm) |> pull())
body_mass_mean_Adelie <- mean(train |> filter(species == "Adelie") |> select(body_mass_g) |> pull())
body_mass_sd_Adelie <- sd(train |> filter(species == "Adelie") |> select(body_mass_g) |> pull())

bill_length_mean_notAdelie <- mean(train |> filter(species == "notAdelie") |> select(bill_length_mm) |> pull())
bill_length_sd_notAdelie <- sd(train |> filter(species == "Adelie") |> select(bill_length_mm) |> pull())
bill_depth_mean_notAdelie <- mean(train |> filter(species == "Adelie") |> select(bill_depth_mm) |> pull())
bill_depth_sd_notAdelie <- sd(train |> filter(species == "Adelie") |> select(bill_depth_mm) |> pull())
flipper_length_mean_notAdelie <- mean(train |> filter(species == "Adelie") |> select(flipper_length_mm) |> pull())
flipper_length_sd_notAdelie <- sd(train |> filter(species == "Adelie") |> select(flipper_length_mm) |> pull())
body_mass_mean_notAdelie <- mean(train |> filter(species == "Adelie") |> select(body_mass_g) |> pull())
body_mass_sd_notAdelie <- sd(train |> filter(species == "Adelie") |> select(body_mass_g) |> pull())
```

### My answer

Each attribute‚Äôs means and standard deviations separately in the training set for both classes are as follows:

```{r}
train |> 
  group_by(species) |> 
  summarise(
    mean_bill_length_mm = mean(bill_length_mm),
    sd_bill_length_mm = sd(bill_length_mm),
    mean_bill_depth_mean_mm = mean(bill_depth_mm),
    sd_bill_depth_mean_mm = sd(bill_depth_mm),
    mean_flipper_length_mm = mean(flipper_length_mm),
    sd_flipper_length_mm = sd(flipper_length_mm),
    mean_body_mass_g = mean(body_mass_g),
    sd_body_mass_g = sd(body_mass_g)
  )
```

The class probabilities using Laplace smoothing with a pseudocount of 1 on the training set are as follows:

```{r}
# assign values 
pseudocount <- 1
n_trial <- nrow(train)
class <- table(train$species)
n_dimension <- class |> length()
# calculate 
class_probs <- (class + pseudocount) / (n_trial + n_dimension)
```

```{r}
total_count <- nrow(train)
class_counts <- table(train$species)
class_probs <- (class_counts + 1) / (total_count + length(class_counts))
```

Class probabilities of `r names(class)[1]` is
`r class_probs[[1]] |> round(3)`; and class probabilities of
`r names(class)[2]` is `r class_probs[[2]] |> round(3)`

## Problem 10 Task b

### Question
 
Now, you can find the class-specific expressions for $p(x|y)$ needed by the NB classifier. Remember that according to NB assumption, the dimensions are independent, and hence, you can represent the class-specific $p(x|y)$ likelihoods as products of 4 1-dimensional normal distributions. Write down the formula needed to compute the posterior probability of the class being Adelie $\hat{p}(y = Adelie | x)$) as a function of the four measurements in x and the statistics (means, standard deviations, class probabilities) you computed in the task a above.

### My answer

The class-specific likelihood $p(x|y)$ is represented as the product of four 1-dimensional normal distributions, one for each feature.

Given $p(x_i|y)$ follows a normal distribution with mean $\mu_{yi}$ and standard deviation $\sigma_{yi}$ for feature $i$ and and class $y$.

Class probability $p(y)$ are estimated from the data.

$x = (x_1, x_2, x_3, x_4)^T$ are four measurements.

The posterior probability $\hat{p}(y = Adelie| x)$ is calculated as follows:

$$
\hat{p}(y = \text{Adelie} | x) = \frac{p(x | y = \text{Adelie}) \cdot p(y = \text{Adelie})}{p(x)}
$$

Where, 

$$
p(x | y = \text{Adelie}) = \prod_{i=1}^{4} \frac{1}{\sqrt{2\pi\sigma_{\text{Adelie},i}^2}} \exp\left(-\frac{(x_i - \mu_{\text{Adelie},i})^2}{2\sigma_{\text{Adelie},i}^2}\right)
$$

Note that $\mu$ and $\sigma$ for each i have been calculated in the previous question.

And,

$p(y = \text{Adelie})$ has been calculated using Laplace smoothing in the previous question.

And, 

$$
p(x) = \left( \prod_{j=1}^{4} \frac{1}{\sqrt{2\pi\sigma_{\text{Adelie},j}^2}} e^{-\frac{(x_j - \mu_{\text{Adelie},j})^2}{2\sigma_{\text{Adelie},j}^2}} \right) p(\text{Adelie}) + \left( \prod_{j=1}^{4} \frac{1}{\sqrt{2\pi\sigma_{\text{notAdelie},j}^2}} e^{-\frac{(x_j - \mu_{\text{notAdelie},j})^2}{2\sigma_{\text{notAdelie},j}^2}} \right) p(\text{notAdelie})

$$
Note that in our case, it is possible to assign class without calculating this $p(x)$ because it's the same for both classes and thus use it as denominator will not change the order of size for the nominators. 

## Problem 10 Task c

### Question

Using the formula you derived in Task b, compute and report your classifier‚Äôs classification accuracy on the test set. Additionally, calculate and report the probabilities $\hat{p}( y = Adelie| x)$ for the three first penguins in the test set.

```{r}
#a function to calculate posterior for the row that is specified
calc_posterior_adelie <- function(x, row) {
  #likelihood_nominator_product <- 1
  likelihood_denominator_product_y1 <- 1
  likelihood_denominator_product_y2 <- 1
  for (i in 1:4) {
    # Calculate the likelihood for each feature and multiply them together
    # likelihood_nominator_product <- likelihood_nominator_product * 
    #                       dnorm(
    #                         x[row, i] |> pull(), 
    #                         mean(x |> filter(species == "Adelie") |> select(i) |> pull()), 
    #                         sd(x |> filter(species == "Adelie") |> select(i) |> pull())
    #                         )
    
    likelihood_denominator_product_y1 <- likelihood_denominator_product_y1*
      dnorm(x[row, i] |> pull(), 
            mean(train |> filter(species == "Adelie") |> select(i) |> pull()), 
            sd(train |> filter(species == "Adelie") |> select(i) |> pull())
                        )
    
    likelihood_denominator_product_y2 <- likelihood_denominator_product_y2*
      dnorm(x[row, i] |> pull(),
            mean(train |> filter(species == "notAdelie") |> select(i) |> pull()), 
            sd(train |> filter(species == "notAdelie") |> select(i) |> pull())
    )
  }
  

  p_x <- likelihood_denominator_product_y1 * class_probs[[1]] + likelihood_denominator_product_y2 * class_probs[[2]]
  # Calculate the  posterior probability
  posterior <- (likelihood_denominator_product_y1 * class_probs[[1]])/p_x
  return(posterior)  # Note: This is unnormalized
}

# predict testing set
test$predicted_prob <- sapply(1:nrow(test), function(i)calc_posterior_adelie(test, i) |> round(3))
test$predicted_class <- sapply(1:nrow(test), function(i) {
  x <- test
  posterior_prob_adelie <- calc_posterior_adelie(test, i)
  if (posterior_prob_adelie >= 0.5) "Adelie" else "notAdelie"
})
```



```{r}
accuracy <- mean(test$predicted_class == test$species)
```

My classifier‚Äôs classification accuracy on the test set is `r accuracy``

The probabilities $\hat{p}(y = \text{Adelie} | x)$ for the three first penguins are as follows:

```{r}
test[1:3,c(5:7)]
```

# Problem 11

Objective: Understanding discriminative vs generative learning.

Download the reference below. You do not need to read the full paper or understand all the details! Instead, try to find the answers to the following questions.

Reference: Ng, Jordan (2001) On discriminative vs. generative classifiers: A comparison of logistic regres- sion and naive Bayes. NIPS. http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers- a-comparison-of-logistic-regression-and-naive-bayes.pdf

## Problem 11 Task a

### Question

Read the Abstract and Introduction (Sect. 1). According to the authors, is discriminative learning better than generative learning? Justify your answer.

### My answer

## Problem 11 Task b

### Question

By a ‚Äúparametric family of probabilistic models‚Äù, the authors mean a set of distributions where a group of parameters defines each distribution. An example of such a family is the family of normal distributions where the parameters are $\mu$ and $\Sigma$.

Ng and Jordan denote by hùê∫ùëíùëõ and hùê∑ùëñùë† two models chosen by optimizing different objectives. Which two families do the authors discuss, and what are the ($h_{Gen}$, $h_{Dis}$) pairs for those models? What objectives are being optimised?

### My answer

## Problem 11 Task c

### Question

Study Figure 1 in the paper. Explain what it suggests (see the last paragraph of the Introduction). Reflect on what this means for the families in Task b.

### My answer



# Problem 12

Objective: comparing classifiers on synthetic data, application of different classifiers. In this problem, you will compare different classifiers using synthetic toy data sets. 

Section 4.7.2 (Logistic regression) and 4.7.5 (NB) of ISLR_v2 (or ISLP) contain helpful information for solving this problem.

Toy data sets

We have generated ten training data sets of different sizes toy_train_<n>.csv for $n \in {23, 25, ..., 212}$, and one test data set toy_test.csv with 10000 points.

Each toy data set has a binary class variable $y \in {0,1}$and two real-valued features $x_1$, $x_2 \in R$. The data are generated from the ‚Äútrue‚Äù model as follows:

  - x_1 and x_2 are sampled from a normal distribution with zero mean and unit variance.
  
  - The probability of ùë¶ is given by:
  
  $$
  p(y=1|x_1, x_2) = \sigma(-\frac{1}{2}-x_1 + 3 \times \frac{x_2}{2} + \frac{x_1 \times x_2}{3} )
  $$

where $\sigma(t) = 1/(1 + e^{-t})$is the standard logistic function.

## Problem 12 Task a

### Question

Is the Naive Bayes (NB) assumption valid for the toy data set? Explain why or why not.

### My Answer

NOT valid. The Naive Bayes (NB) assumption states that the features in a data are conditionally independent given the class label. In the formula of how y is generated, there is a interaction term $\frac{x_1 \times x_2}{3}$, which means the effect of $x_1$ on $y$ is modified by the effect of $x_2$ on $y$. That is, they are not independent on each other. 

## Problem 12 Task b

### Question 

For each training set, train several classifiers that output probabilities (described below), and then report their accuracy and perplexity on the test set.
Produce the following table (or make a plot) for accuracy and for perplexity on the test set:

```{r}
p12.tab <- matrix(rep("?", 60), nrow = 10, ncol = 6)
colnames(p12.tab) <- c("n", "NB", "LR", "iLR", "Optimal Bayes", "Dummy")
p12.tab <- p12.tab |> data.frame()
for (i in 1:nrow(p12.tab)){
  p12.tab[i, "n"] <- 2^i
}
p12.tab |> kable()
```


where the columns correspond to:
 - Naive Bayes (NB) (e.g., naive Bayes from the library e1071)
 - Logistic regression without an interaction term (e.g., glm)
 - Logistic regression with an interaction term (e.g., glm)
 - Optimal Bayes classifier that uses the actual class conditional probabilities (that you know in this case!)
to compute $p(y|x_1,x2)$ for a given $(x_1,x_2)$ - no probabilistic classifier can do better than this.

`Dummy classifier` that does not depend on x. It always outputs the probability ùëùÃÇ(ùë¶ = 1 ‚à£ ùë•1,ùë•2) as the fraction of ùë¶ = 1 in the training data. ‚ÄúDummy‚Äù means that the classifier output does not depend on the covariates. Including a dummy classifier in your comparison is always a good idea! One way to get a dummy classifier here is to train a logistic regression with only the intercept term.

### My answer

```{r}
library(e1071)
library(glmnet)

# Initialize the results table
p12.tab <- matrix(rep("?", 60), nrow = 10, ncol = 6)
colnames(p12.tab) <- c("n", "NB", "LR", "iLR", "Optimal Bayes", "Dummy")
p12.tab <- p12.tab |> data.frame()
for (i in 1:nrow(p12.tab)) {
  p12.tab[i, "n"] <- 2^(i + 2)
}
```

```{r}
#read data
test_data <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "toy_test.csv"))
```

```{r}
# Function to calculate the probability using the Optimal Bayes classifier
optimal_bayes_prob <- function(x1, x2) {
  t <- -1/2 - x1 + 3 * x2 / 2 + x1 * x2 / 3
  return(1 / (1 + exp(-t)))
}


```


```{r}
# Define a function to calculate accuracy and perplexity
calculate_metrics <- function(predicted_prob, actual) {
  # Calculate accuracy
  predicted_class <- ifelse(predicted_prob >= 0.5, 1, 0)
  accuracy <- mean(predicted_class == actual)

  # Calculate perplexity
  perplexity <- exp(-mean(actual * log(predicted_prob) + (1 - actual) * log(1 - predicted_prob)))

  return(c(accuracy, perplexity))
}

# Loop over each training set size
for (i in 1:nrow(p12.tab)) {
  n <- p12.tab[i, "n"]
  
  # Load the training data
  train_data <- read.csv(paste0("/Users/rongguang/Documents/Projects/Machine Learning/Exercise Sets-20231117/E2/data_E2/toy_train_", n, ".csv"))

  # Naive Bayes
  NB_model <- naiveBayes(y ~ ., data = train_data)
  NB_prob <- predict(NB_model, test_data, type = "raw")[, 2]
  p12.tab[i, c("NB", "NB_perp")] <- calculate_metrics(NB_prob, test_data$y)

  # Logistic Regression without interaction term
  LR_model <- glm(y ~ ., data = train_data, family = binomial)
  LR_prob <- predict(LR_model, test_data, type = "response")
  p12.tab[i, c("LR", "LR_perp")] <- calculate_metrics(LR_prob, test_data$y)

  # Logistic Regression with interaction term
  iLR_model <- glm(y ~ . + x1 * x2, data = train_data, family = binomial)
  iLR_prob <- predict(iLR_model, test_data, type = "response")
  p12.tab[i, c("iLR", "iLR_perp")] <- calculate_metrics(iLR_prob, test_data$y)

  # Optimal Bayes
  test_data$optimal_bayes_prob <- with(test_data, optimal_bayes_prob(x1, x2))
  optimal_bayes_true_metrics <- calculate_metrics(test_data$optimal_bayes_prob, test_data$y)
  p12.tab[i, c("Optimal.Bayes", "Optimal.Bayes_perp")] <- optimal_bayes_true_metrics
  # Dummy classifier
  dummy_prob <- mean(train_data$y)
  dummy_metrics <- calculate_metrics(rep(dummy_prob, nrow(test_data)), test_data$y)
  p12.tab[i, c("Dummy", "Dummy_perp")] <- dummy_metrics
}

```

The table of accuracy is as follows:

```{r}
p12.tab |> 
  dplyr::select(
    !contains("_perp")
  )
```

The table of perplexity is as follows:

```{r}
p12.tab |> 
  dplyr::select(
    n, contains("_perp")
  )
```

## Problem 12 Task c

### Question

Task c
Report the logistic regression coeÔ¨Äicients with interaction terms for the largest training data set. How do they compare with the coeÔ¨Äicients of the actual model that generated the data?
Discuss your observations and what you can conclude.

  - Which of the models above are probabilistic, discriminative, and generative?
  
  - How do accuracy and perplexity (log-likelihood) compare?
  
  - Is there a relation to the insights from the previous problem?
  
  - Why does logistic regression with the interaction term perform so well for larger datasets?
  
  - Does your dummy classifier ever outperform other classifiers, or do different classifiers outperform the optimal Bayes classifier?

### My answer 

This is the logistic regression coeÔ¨Äicients with interaction terms for the largest training data set:

```{r}
# Assuming the largest dataset is loaded as largest_train_data
# The model formula includes interaction between x1 and x2
largest_train_data <- read.csv(paste0("/Users/rongguang/Documents/Projects/Machine Learning/Exercise Sets-20231117/E2/data_E2/toy_train_", "4096", ".csv"))
model <- glm(y ~ x1 + x2 + I(x1 * x2), data = largest_train_data, family = binomial())
print(coef(model))
```


In the exercise, 

  - Logistic regression, optimal Bayes classifiers, and naive Bayesian probabilistic; 
  
  - Logistic Regression (both with and without interaction terms) is a discriminative model.
  
  - Naive Bayes is a generative model.

Accuracy vs perplexity,

  - Accuracy measures the proportion of correct predictions.
  
     - Higher accuracy, more correct classification. 
  
  - Perplexity is a measure of how well the probability model predicts a sample.

     - A less perplexed model is more confident and accurate in its predictions.
     
  - In our exercise, accuracy increases with the sample size increasing and hit the ceiling around hundreds.
  
  - In our exercise, perplexity decreases with the sample size increasing and hit the ceiling around hundreds. 
  
  - In our exercise, lower perplexity model usually outperforms higher perplexity model in terms of accuracy, and higher accuracy model usually outperforms lower perplexity model in terms of perplexity.
  

Why logistic regression with interaction term works well?

   - Because it captures the interactive relationship between $x_1$ and $x_2$ in the model, which is pre-defined in the true model in our exercise.

   - With increasing sample size, model variance is decreasing. Such pattern of interaction is more pronounced. 

Does your dummy classifier ever outperform others?

   - No. Generally it works worse than other classifiers.
   
   - Exceptions are when the sample sizes are extremely small/
   
   
Do different classifiers outperform the optimal Bayes classifier?

   - No. Optimal Bayes classifier uses the true model and it generally outperforms other classifiers.
   
   - Exceptions are one iLR classifier with large sample size by very small margin, which might be due to over-fitting. With even larger sample, this edge disappears.  

# Problem 13

Objectives: basic principles of decision trees

In this task, you will simulate a decision tree algorithm by hand using the toy data shown in the figure. Read Section 8.1 of ISLR_v2. Use the Gini index of Equation (8.6) as an impurity measure.

(You do not need to worry about overfitting here: the resulting classification tree should have enough splits to fit the training data without error. Don‚Äôt worry if your results are not optimal or super-accurate, as long as they are ‚Äúin the ballpark‚Äù.)

## Problem 13 Task a


