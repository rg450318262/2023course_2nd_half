---
title: "E2 assignment"
output: 
    html_document
date: "2023-11-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, warning = F, message = T, echo = T)
```

# Problem 8

Performance measures In this exercise set, you will train probabilistic
classifiers which estimate $\hat{p}(y | x)$: the probability of class
$y$ given the covariate vector x. Two commonly used performance measures
for probabilistic classifiers are accuracy and perplexity. We use the
following notation:

-   $y_i \in \{0,1\}$: the true class of point $i$.

-   $\hat{p} = \hat{p}(y = 1 | x)$: the estimated probability for the
    $ith$ point in a dataset of size $n$ being spam.

-   $\hat{y}$: the predicted class for point i which is $\hat{y} = 1$ if
    $\hat{p} \geq 0.5$ and $\hat{y} = 0$.We define the accuracy on a
    dataset of n items as follows:

$$
accuracy = \Sigma_{i=1}^{n}I(y = \hat{y_i})/n
$$

$$
perplexity = \exp(-\Sigma_{i=1}^{n}log\hat{p}(y = y_i|x_i)/n)
$$

Perplexity is a transformation of the likelihood (perplexity =
exp($-loglikelihood/n$)), which may be the most commonly used
performance measure on probabilistic classifiers. Example values are
perplexity = 1 for a perfect classifier, which always predicts the
probability of oneto an actual class, and perplexity = 2 for coin
flipping, which has a predicted class probabilit $\hat{p} = 1/2$.

## Problem 8 Task a

### Question

Using one-hot encoding for $y_i$, train a logistic regression model
without Lasso or Ridge regularisation on the training data. Then: (i)
Report the model coeï¬€icients. (ii) Compute and report the accuracy and
perplexity on the training and testing data. Make sure that you use no
regularisation. Notice that you may get warnings about convergence; why?
(iii) Write down the equation for the predicted class probability, given
the model coeï¬€icients $\beta$ and the covariate vector $x$.

### My answer

```{r}

library(tidyverse)
library(here)

#read data
train.set <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "spam_train.csv"))

test.set <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "spam_test.csv"))
```

**The table below shows the coefficients for the model without
regularization.**

```{r}
library(kableExtra)
# train.set |> map(is.na) |> map(sum)
# test.set |> map(is.na) |> map(sum)
```

```{r waning = T, message=T}
#load packages
library(broom)
library(kableExtra)
library(glmnet)
#train model on train set
fit <- glm(SPAM ~ ., data = train.set, family = binomial(link ="logit"))

#check output (model coefficients)
coef(fit) |> 
  tidy() |> 
  rename(
    variable = names,
    coefficient = x) |>
  mutate_if(is.numeric, ~round(.x, 3)) |> 
  DT::datatable()
#caption = "Fig P1_Task_a_1. Model coefficient on train set"

```

**The table below shows the accuracy on training and testing set.**

```{r}
# function to derive accuracy

get_accuracy <- 
  function(fit.train, test.set, true.y){
    
    # Train accuracy 
  
    ##get sample size
    n_train <-  nrow(fit.train$model[1]) #1 is list of true y 
    ##get true y
    y_train <- fit.train$model[1]
    ##get predicted probability
    prob_train <- fit.train$fitted.values
    ## assign probability to class(1/0)
    class_train <- ifelse(prob_train>=0.5, 1, 0)
    ## indicator function (0/1) for test
    indicator_train <- ifelse(class_train == y_train, 1, 0)
    ##accuracy
    accuracy_train = sum(indicator_train/n_train)
    
    # Test accuracy
    
    ## get sample size for test set
    n_test <- nrow(test.set)
    ## get true y for test set
    y_test <- test.set[,true.y]
    ## get predicted probability 
    prob_test <- predict(fit.train, test.set, type = "response")
    ## assign probability to class(1/0)
    class_test <- if_else(prob_test>=0.5, 1, 0)
    ## indicator function (0/1) for test
    indicator_test <- if_else(class_test == y_test, 1, 0)
    ## accuracy for test
    accuracy_test <- sum(indicator_test/n_test)
    
    ##print results
    output <- data.frame(accuracy_train = accuracy_train, accuracy_test =accuracy_test)
    print(output)
  }

# get_accuracy(fit, test.set, "SPAM") |> 
#   DT::datatable()
baseline_test_accuracy<- get_accuracy(fit, test.set, "SPAM")[1,2]
```

**The table below shows the perplexity on training and testing set.**

```{r}
# function to derive perplexity

get_perplexity <-  #true.y is the variable name of y in test.set
  function(fit.train, test.set, true.y){
    
    # Train perplexity 
  
    ##get sample size
    n_train <-  nrow(fit.train$model[1]) #1 is list of true y 
    ##get true y
    y_train <- fit.train$model[1]
    ##get predicted probability
    prob_train <- fit.train$fitted.values
    ##Indicator function (0/1)
    indicator_train <- if_else(prob_train>=0.5, 1, 0)
    ##log likelihood for train
    log_likelihood_train <- sum(y_train*log(prob_train) + (1-y_train)*log(1-prob_train))
    ##perplexity for train
    perplexity_train <- exp(-log_likelihood_train/n_train)
    
    
    # Test perplexity

    ## get sample size for test set
    n_test <- nrow(test.set)
    ## get true y for test set
    y_test <- test.set[, true.y] 
    ## get predicted probability 
    prob_test <- predict(fit.train, test.set, type = "response")
    ## og likelihood for test
     log_likelihood_test <- sum(y_test*log(prob_test) + (1-y_test)*log(1-prob_test))
    ## perplexity for test
    perplexity_test<- exp(-log_likelihood_test/n_test)
    
    ##print results
    output <- data.frame(perplexity_train = perplexity_train, perplexity_test =perplexity_test)
    print(output)
  }

# generate the table 
# p1.task.a.table.1 <- get_perplexity(fit, test.set, "SPAM") |>
#   mutate_if(is.numeric, ~round(.x,3))
# p1.task.a.table.1|> 
#   DT::datatable()

#get values to report
baseline_test_perplexity <- get_perplexity(fit, test.set, "SPAM")[1,2]

```

**Why don't converge?** The warning of convergence can be the result of
overly complex model, multi-colinearity and too few observations in
terms of predictors. In our case, we have fairly small number of
predictors against observations. I suppose the problem is mostly due
tomulti-colinearity. This can be further consolidated by the common
factthat a spam email usually has more than one of these identifying
features. To tell the truth, I say it mostly because task b is about
regularization. Couldn't find a safer guess. :)

**Below is the equation for the predicted class probability, given the
model coeï¬€icients** $\beta$ and the covariate vector $x$.

$$
\hat{p}(y = 1 \mid x) = \frac{1}{1 + e^{-(-55.0 + 17.5 \times x_1 + 0 \times x_2+ 17.9 \times x_3 + 34.9 \times x_4 + 38.1 \times x_5)}} \\
$$

Where $x_1$ is MISSING_FROM; $x_2$ is FROM_ADDR_WS; $x_3$ is
TVD_SPACE_RATIO; $x_4$ is LOTS_OF_MONEY; $x_5$ is
T_FILL_THIS_FORM_SHORT.

## Problem 8 Task b

### Question

Train a logistic regression model with Lasso regularisation. Find a
regularisation coeï¬€icient value that performs better than the
unregularised version on the test data and has some regression
coeï¬€icients equal to zero. You can do this by trying various values;
there is no need to be more sophisticated here. Report your parameters,
the regression coeï¬€icients, and the accuracies and perplexities on the
testing data. Look at the predicted class probabilities for the
unregularized regressor in Task a, and your regularized regressors in
Task b. How do the distributions of the probabilities differ?

### My answer

**I trained LASSO models with a series of lambda values using train
dataset.** Then I predicted the probability of SPAM by trying different
lambda values including $\lambda$ = 0.0005, 0.05, 0.5 and 5. I selected
these $\lambda$ values arbitrarily. The table below shows their
parameters, accuracy (test set) and perplexity(test set). The first row
also exhibits un-regularized model created in task a, for the purpose of
comparing. I noticed that the lambda = 0.0005 model and un-regularized
model have same level of testing accuracy, but the former outperforms in
terms of much smaller perplexity value. Actually, the un-regularized
model's perplexity indicates it is worse than random guess. So to answer
the next part of question, I will visualize their probabilities

```{r}
# extract train set x frame
train.set.x <- 
  train.set[,which(colnames(train.set) != "SPAM")] |> 
  as.matrix()
# extract train set y vedtor
train.set.y <- 
  train.set$SPAM 

# extract test set x frame
test.set.x <- 
  test.set[,which(colnames(train.set) != "SPAM")] |> 
  as.matrix()
# extract est set y vedtor
test.set.y <- 
  test.set$SPAM 

#define lambada values to test out
grid <- 10^seq(10, -2, length = 100)

#fit a model with lasso regularization
fit.regular <- 
  glmnet(
    train.set.x, 
    train.set.y, 
    alpha = 1, #lasso
    lambda = grid,  
    family = "binomial"
    )
```

```{r}
#try various lambda values

## try lambda = 0
prediction.l0 <-  
  predict(
    fit.regular, 
    s = 0, 
    newx = test.set.x,
    type = "response"
    )

## try lambda = 0.0005
prediction.l00005 <-  
  predict(
    fit.regular, 
    s = 0.0005, 
    newx = test.set.x,
    type = "response"
    )

## try lambda = 0.05
prediction.l005 <-  
  predict(
    fit.regular, 
    s = 0.05, 
    newx = test.set.x,
    type = "response"
    )
## try lambda = 0.5
prediction.l05<-  
  predict(
    fit.regular, 
    s = 0.5, 
    newx = test.set.x,
    type = "response"
  )
## try lambda = 5
prediction.l5<-  
  predict(
    fit.regular, 
    s = 5, 
    newx= test.set.x,
    type = "response"
    )

prd.lm.s005 <-  
  predict(
    fit.regular, 
    s = 0.000026, 
    newx= test.set.x,
    type = "response"
    )
```

```{r}
# obtain accuracy and perplexity for the models created

#lambda = 0
## accuracy
predicted_classes <- ifelse(prediction.l0 > 0.5, 1, 0)
accuracy_test_l0 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l0 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l0), log(1 - prediction.l0))))
## Coefficient
coef.l0 <-  
  predict(
    fit.regular, 
    s = 0, 
    newx = test.set.x,
    type = "coefficient"
    )
#lambda = 0.0005
## accuracy
predicted_classes <- ifelse(prediction.l00005 > 0.5, 1, 0)
accuracy_test_l00005 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l00005 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l00005), log(1 - prediction.l00005))))
## Coefficient
coef.l00005 <-  
  predict(
    fit.regular, 
    s = 0.0005, 
    newx = test.set.x,
    type = "coefficient"
    )

#lambda = 0.05
##accuracy
predicted_classes <- ifelse(prediction.l005 > 0.5, 1, 0)
accuracy_test_l005 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l005 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l005), log(1 - prediction.l005))))
## Coefficient
coef.l005 <-  
  predict(
    fit.regular, 
    s = 0.05, 
    newx = test.set.x,
    type = "coefficient"
    )
#lambda = 0.5
##accuracy
predicted_classes <- ifelse(prediction.l05 > 0.5, 1, 0)
accuracy_test_l05 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l05 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l05), log(1 - prediction.l05))))
## Coefficient
coef.l05 <-  
  predict(
    fit.regular, 
    s = 0.5, 
    newx = test.set.x,
    type = "coefficient"
    )

#lambda = 5
## accuracy
predicted_classes<- ifelse(prediction.l5 > 0.5, 1, 0)
accuracy_test_l5 <- mean(predicted_classes == test.set.y)
## perplexity 
perplexity_test_l5 <- 
  exp(-mean(ifelse(test.set.y == 1, log(prediction.l5), log(1 - prediction.l5))))
## Coefficient
coef.l5 <-  
  predict(
    fit.regular, 
    s = 5, 
    newx = test.set.x,
    type = "coefficient"
    )

coefficient.l00005 <-  
  predict(
    fit.regular, 
    s = 0.0005, 
    newx = test.set.x,
    type = "coefficient"
    )
```

```{r}
p1_taskb_table <- 
  tibble(
    model = c(
      "no regularization",
      #"lambda = 0",
      "lambda = 0.0005",
      "lambda = 0.05",
      "lambda = 0.5",
      "lambda = 5"),
    `accuracy(test set)` = c(
      baseline_test_accuracy, 
      #accuracy_test_l0, 
      accuracy_test_l00005,
      accuracy_test_l005,
      accuracy_test_l05,
      accuracy_test_l5
      ), 
    `perplexity(test set)` = c(
      baseline_test_perplexity, 
      #perplexity_test_l0, 
      perplexity_test_l00005,
      perplexity_test_l005,
      perplexity_test_l05,
      perplexity_test_l5
      )
  )

#coef.l0.parameters <- coef.l0 |> as.matrix() |> data.frame() 
coef.l00005.parameters <- coef.l00005 |> as.matrix() |> data.frame() 
coef.l005.parameters <- coef.l005 |> as.matrix() |> data.frame() 
coef.l05.parameters <- coef.l05 |> as.matrix() |> data.frame() 
coef.l5.parameters <- coef.l5 |> as.matrix() |> data.frame() 
baseline <- fit$coefficients |> data.frame()
parameters <- 
  cbind(
    baseline,
    #coef.l0.parameters,
    coef.l00005.parameters,
    coef.l005.parameters, 
    coef.l05.parameters,
    coef.l5.parameters
    ) 

colnames(parameters) <- c(
  #"l0", 
  "l00005", "l005", "l05", "l5")
parameter.table <- parameters |> t() |> data.frame()
rownames(parameter.table) <- NULL
p1_taskb_table <- cbind(p1_taskb_table, parameter.table)
```

```{r}
p1_taskb_table |> 
  mutate_if(is.numeric, ~round(.x,3)) |> 
  DT::datatable()

```

**To analyze how the distributions of probabilities differ between the
regularized and unregularized models, I visualized and overlaid their
density plots.** On each side of p>0.5 ($\hat{y} = 1$) and
p<0.5$\hat{y} = 0$, the LASSO model with $\lambda = 0.0005$ shows more
centered and higher peaks.

```{r}
plot(density(prediction.l00005), col = "blue", cex.main = 0.9, main = "Distribution of probability non-regularized model vs Lasso model (lambda = 0.0005)", xlab = "Predicted probability")
lines(density(fit$fitted.values), col = "red")
abline(v = 0.5, col = "black")
text(0.7, 2, "<-Classifiction cut off")
text(-0.15, 2.7, "predicted_y = 0", cex = 0.8)
text(0.65, 2.7, "predicted_y = 1", cex = 0.8)

legend("topright", c("Non-regularized model", "Lasso model"), 
       col =c("blue","red"), cex =0.7, lty=1)
```

# Problem 9

[6 points] Objective: generative Bayes classifier

In this problem, you will study the quadratic discriminant analysis
(QDA) classifier. Consider a simple case with two classes and only one
feature ($K$ = 2 and $p$ = 1).

## Question Task a

Prove that the QDA classifier is not linear if the class-specific
variances differ ($\sigma_1^2 \ne \sigma_2^2$).

Hint: This problem is from the textbook (Problem 3, page 189). Please
see the discussion in the textbook for hints and guidance. For this
problem, you should follow the arguments laid out in Sect. 4.4.1 of the
textbook, but without assuming that $\sigma_1^2 = \sigma_2^2$

## My answer

The bayesian therom states that

$$
p(y=k|x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)} ..........(1)
$$

In the one-normal dimensional setting, the normal density takes the form: 

$$
f_k(x) = \frac{1}{\sqrt{2\pi\sigma_k^2}} \exp\left(-\frac{(x - \mu_k)^2}{2\sigma_k^2}\right)......(2)
$$

Substitute (2) into (1)then we get (note that here $\sigma_1^2 \ne \sigma_2^2$):

$$
p(y=k|x) = \frac{\pi_k \frac{1}{\sqrt{2\pi\sigma_k^2}} \exp\left(-\frac{(x - \mu_k)^2}{2\sigma_k^2}\right)}{\sum_{l=1}^{K} \pi_l \frac{1}{\sqrt{2\pi\sigma_l^2}} \exp\left(-\frac{(x - \mu_l)^2}{2\sigma_l^2}\right)}
$$

Take the logarithm of the posterior probability and ignore the denominator we get:

$$
\delta_k(x) = \log(\pi_k) - \frac{1}{2}\log(2\pi\sigma_k^2) - \frac{(x - \mu_k)^2}{2\sigma_k^2}
$$
Then we get

$$
\delta_k(x) = \log(\pi_k) - \frac{1}{2}\log(\sigma_k^2) - \frac{(x - \mu_k)^2}{2\sigma_k^2}
$$

As such, the QDA classifier is not linear if the class-specific variances differ ($\sigma_1^2 \ne \sigma_2^2$).

# Problem 10

Objective: naive Bayes classifier

```{r}
library(magick)
image_read("https://const-ae.name/img/penguins_vs.png") |>
  image_convert("svg")
```


In this problem, you will study the Palmer penguins by building your own
Naive Bayes classifier.

Artwork by @allison_horst

-   Dataset description.

-   Use penguins_train.csv as your training data and penguins_test.csv
    as your testing data.

-   Binary classification task: classify the penguin species as
    $y \in {Adelie, notAdelie}$ (not Adelie combining Gentoo and
    Chinstrap species) based on four morphological and weight
    measurements of the individual penguins, denoted by
    $x=(x_1, x_2, x_3, x_4)^T \in \mathbb{R}_4$

Naive Bayes (NB) classifier

Your task is to build your own NB classifier; you should not use a
ready-made classifier from a library. However, you do not need to create
a generic classifier (such as naive Bayes in the R e1071 library); it is
enough that your classifier works for this particular task.

The idea of NB is that the dimensions are conditionally independent,
given the class. Each class conditional feature distribution $p(x_i|y)$
is assumed to originate from an independent Gaussian distribution with
its mean $\mu$ and variance $\sigma^2$ for $i = 1,2,3,4$

## Problem 10 Task a

### Question

Compute and report each attribute's means and standard deviations
separately in the training set for both classes.

Estimate and report the class probabilities using Laplace smoothing with
a pseudocount of 1 on the training set. (You should produce a total of
18 numbers from this task.)

```{r}
# read train set
train <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "penguins_train.csv"))
# read test set
test <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "penguins_test.csv"))
```

```{r}
bill_length_mean_Adelie <- mean(train |> filter(species == "Adelie") |> select(bill_length_mm) |> pull())
bill_length_sd_Adelie <- sd(train |> filter(species == "Adelie") |> select(bill_length_mm) |> pull())
bill_depth_mean_Adelie <- mean(train |> filter(species == "Adelie") |> select(bill_depth_mm) |> pull())
bill_depth_sd_Adelie <- sd(train |> filter(species == "Adelie") |> select(bill_depth_mm) |> pull())
flipper_length_mean_Adelie <- mean(train |> filter(species == "Adelie") |> select(flipper_length_mm) |> pull())
flipper_length_sd_Adelie <- sd(train |> filter(species == "Adelie") |> select(flipper_length_mm) |> pull())
body_mass_mean_Adelie <- mean(train |> filter(species == "Adelie") |> select(body_mass_g) |> pull())
body_mass_sd_Adelie <- sd(train |> filter(species == "Adelie") |> select(body_mass_g) |> pull())

bill_length_mean_notAdelie <- mean(train |> filter(species == "notAdelie") |> select(bill_length_mm) |> pull())
bill_length_sd_notAdelie <- sd(train |> filter(species == "Adelie") |> select(bill_length_mm) |> pull())
bill_depth_mean_notAdelie <- mean(train |> filter(species == "Adelie") |> select(bill_depth_mm) |> pull())
bill_depth_sd_notAdelie <- sd(train |> filter(species == "Adelie") |> select(bill_depth_mm) |> pull())
flipper_length_mean_notAdelie <- mean(train |> filter(species == "Adelie") |> select(flipper_length_mm) |> pull())
flipper_length_sd_notAdelie <- sd(train |> filter(species == "Adelie") |> select(flipper_length_mm) |> pull())
body_mass_mean_notAdelie <- mean(train |> filter(species == "Adelie") |> select(body_mass_g) |> pull())
body_mass_sd_notAdelie <- sd(train |> filter(species == "Adelie") |> select(body_mass_g) |> pull())
```

### My answer

Each attribute's means and standard deviations in the training set for both classes are tablated in the tables as follows:

```{r}
p10.table <- 
train |> 
  group_by(species) |> 
  summarise(
    mean_bill_length_mm = mean(bill_length_mm),
    sd_bill_length_mm = sd(bill_length_mm),
    mean_bill_depth_mean_mm = mean(bill_depth_mm),
    sd_bill_depth_mean_mm = sd(bill_depth_mm),
    mean_flipper_length_mm = mean(flipper_length_mm),
    sd_flipper_length_mm = sd(flipper_length_mm),
    mean_body_mass_g = mean(body_mass_g),
    sd_body_mass_g = sd(body_mass_g)
  ) |> 
  mutate_if(is.numeric, ~round(.,3)) 

p10.table |> 
  select(contains("mean_"))|> 
  DT::datatable()

p10.table |> 
  select(contains("sd_"))|> 
  DT::datatable()
```

The class probabilities using Laplace smoothing with a pseudocount of 1on the training set are as follows:

```{r}
# assign values 
pseudocount <- 1
n_trial <- nrow(train)
class <- table(train$species)
n_dimension <- class |> length()
# calculate 
class_probs <- (class + pseudocount) / (n_trial + n_dimension)
```

```{r}
total_count <- nrow(train)
class_counts <- table(train$species)
class_probs <- (class_counts + 1) / (total_count + length(class_counts))
class_probs |> as.data.frame() |> 
  mutate_if(is.numeric, ~round(.,3)) |> 
  DT::datatable()
```

## Problem 10 Task b

### Question

Now, you can find the class-specific expressions for $p(x|y)$ needed by
the NB classifier. Remember that according to NB assumption, the
dimensions are independent, and hence, you can represent the
class-specific $p(x|y)$ likelihoods as products of 4 1-dimensional
normal distributions. Write down the formula needed to compute the
posterior probability of the class being Adelie
$\hat{p}(y = Adelie | x)$) as a function of the four measurements in x
and the statistics (means, standard deviations, class probabilities) you
computed in the task a above.

### My answer

The class-specific likelihood $p(x|y)$ is represented as the product of
four 1-dimensional normal distributions, one for each feature.

Given $p(x_i|y)$ follows a normal distribution with mean $\mu_{yi}$ and
standard deviation $\sigma_{yi}$ for feature $i$ and and class $y$.

Class probability $p(y)$ are estimated from the data.

$x = (x_1, x_2, x_3, x_4)^T$ are four measurements.

The posterior probability $\hat{p}(y = Adelie| x)$ is calculated as
follows:

$$
\hat{p}(y = \text{Adelie} | x) = \frac{p(x | y = \text{Adelie}) \cdot p(y = \text{Adelie})}{p(x)}
$$

Where,

$$
p(x | y = \text{Adelie}) = \prod_{i=1}^{4} \frac{1}{\sqrt{2\pi\sigma_{\text{Adelie},i}^2}} \exp\left(-\frac{(x_i - \mu_{\text{Adelie},i})^2}{2\sigma_{\text{Adelie},i}^2}\right)
$$

Note that $\mu$ and $\sigma$ for each i have been calculated in the
previous question.

And,

$p(y = \text{Adelie})$ has been calculated using Laplace smoothing in
the previous question.

And,

$$ 
p(x) = 
\left( \prod_{j=1}^{4}*\frac{1}{\sqrt{2\pi\sigma_{\text{Adelie},j}^2}}e^{-\frac{(x_j -\mu_{\text{Adelie},j})^2}{2\sigma_{\text{Adelie},j}^2}}\right) p(\text{Adelie}) + 
\left( \prod_{j=1}^{4}\frac{1}{\sqrt{2\pi\sigma_{\text{notAdelie},j}^2}}e^{-\frac{(x_j - \mu_{\text{notAdelie},j})^2}{2\sigma_{\text{notAdelie},j}^2}}\right) p(\text{notAdelie})
$$ 


Note that in our case, it is possible to assign class without
calculating this $p(x)$ because it's the same for both classes and thus
use it as denominator will not change the order of size for the
nominators.

## Problem 10 Task c

### Question

Using the formula you derived in Task b, compute and report your
classifier's classification accuracy on the test set. Additionally,
calculate and report the probabilities $\hat{p}( y = Adelie| x)$ for the
three first penguins in the test set.

```{r}
#a function to calculate posterior for the row that is specified
calc_posterior_adelie <- function(x, row) {
  #likelihood_nominator_product <- 1
  likelihood_denominator_product_y1 <- 1
  likelihood_denominator_product_y2 <- 1
  for (i in 1:4) {
    # Calculate the likelihood for each feature and multiply them together
    # likelihood_nominator_product <- likelihood_nominator_product * 
    #                       dnorm(
    #                         x[row, i] |> pull(), 
    #                         mean(x |> filter(species == "Adelie") |> select(i) |> pull()), 
    #                         sd(x |> filter(species == "Adelie") |> select(i) |> pull())
    #                         )
    
    likelihood_denominator_product_y1 <- likelihood_denominator_product_y1*
      dnorm(x[row, i] |> pull(), 
            mean(train |> filter(species == "Adelie") |> select(i) |> pull()), 
            sd(train |> filter(species == "Adelie") |> select(i) |> pull())
                        )
    
    likelihood_denominator_product_y2 <- likelihood_denominator_product_y2*
      dnorm(x[row, i] |> pull(),
            mean(train |> filter(species == "notAdelie") |> select(i) |> pull()), 
            sd(train |> filter(species == "notAdelie") |> select(i) |> pull())
    )
  }
  

  p_x <- likelihood_denominator_product_y1 * class_probs[[1]] + likelihood_denominator_product_y2 * class_probs[[2]]
  # Calculate the  posterior probability
  posterior <- (likelihood_denominator_product_y1 * class_probs[[1]])/p_x
  return(posterior)  # Note: This is unnormalized
}

# predict testing set
test$predicted_prob <- sapply(1:nrow(test), function(i)calc_posterior_adelie(test, i) |> round(3))
test$predicted_class <- sapply(1:nrow(test), function(i) {
  x <- test
  posterior_prob_adelie <- calc_posterior_adelie(test, i)
  if (posterior_prob_adelie >= 0.5) "Adelie" else "notAdelie"
})
```

```{r}
accuracy <- mean(test$predicted_class == test$species)
```

My classifier's classification accuracy on the test set is:

```{r}
accuracy
```

The probabilities $\hat{p}(y = \text{Adelie} | x)$ for the three first
penguins are as follows:

```{r}
test[1:3,c(5:7)] |> 
  DT::datatable()
```

# Problem 11

Objective: Understanding discriminative vs generative learning.

Download the reference below. You do not need to read the full paper or
understand all the details! Instead, try to find the answers to the
following questions.

Reference: Ng, Jordan (2001) On discriminative vs. generative
classifiers: A comparison of logistic regres- sion and naive Bayes.
NIPS.
<http://papers.nips.cc/paper/2020-on-discriminative-vs-generative-classifiers->
a-comparison-of-logistic-regression-and-naive-bayes.pdf

## Problem 11 Task a

### Question

Read the Abstract and Introduction (Sect. 1). According to the authors,
is discriminative learning better than generative learning? Justify your
answer.

### My answer

No. Discriminative and generative learning works better in different conditions. According to the authors, contrary to a widely-held belief that discriminative classifiers are almost always to be preferred to generative classifiers, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. 

The justifications are:

1. Conventional belief is discriminative classifiers works better:

a.  Generative learning does not solve the classification task directly.
    Instead, it tries to tap into a more general issue: estimating the
    probability distribution of the response ($y$) given the class
    labels ($x$). According to Occam's Razor, all else being equal, we
    expect simpler hypothesis to perform better than more complicated
    ones. By attempting to solving this general problem, generative
    learning introduces more uncertainty and hence risk lower model
    performance.

b.  For discriminative models, the amount of data required to
    effectively train the model scales linearly with its complexity (VC
    dimension). learn the joint probability distribution $P(x, y)$ and
    typically have higher complexity due to the need to model the
    distribution of each feature. In other word, discriminative models
    are less data-efficient.
    
2. The result from the authors' empirical study gives different results than conventional ideas.

a. In the study, the authors experimented on the performance of naive
   Bayes model (for both discrete and continuous inputs) and its
   discriminative analog, logistic regression/linear classification.

b. The results show: The generative model does indeed have a higher asymptotic error (as the number of
   training examples becomes large) than the discriminative model. This stems from the observation which    is borne out in repeated experiments that while discriminative learning has lower asymptotic error, a
   generative classifier may also approach its (higher) asymptotic error much faster.

## Problem 11 Task b

### Question

By a "parametric family of probabilistic models", the authors mean a set
of distributions where a group of parameters defines each distribution.
An example of such a family is the family of normal distributions where
the parameters are $\mu$ and $\Sigma$.

Ng and Jordan denote by hGen and hDis two models chosen by optimizing
different objectives. Which two families do the authors discuss, and
what are the ($h_{Gen}$, $h_{Dis}$) pairs for those models? What
objectives are being optimised?

### My answer

The two families the authors discussed:  multi-nominal and linear. 

The ($h_{Gen}$, $h_{Dis}$) pairs are a. naive Bayes model and logistic regression; 
  b.  normal discriminant analysis and logistic regression

Optimised objectives include 0-1 training error, joint likelihood of the
inputs and the labels, and conditional likelihood $p(y|x)$,

## Problem 11 Task c

### Question

Study Figure 1 in the paper. Explain what it suggests (see the last
paragraph of the Introduction). Reflect on what this means for the
families in Task b.

### My answer

As the number of training examples is increased, there can be two distinct regimes of performance, the first in which the generative model has already approached its asymptotic error and is thus doing better, and the second in which the discriminative model approaches its lower asymptotic error and does better.

For naive Bayes model and logistic regression family, a generative classifier usually approach its asymptotic error much faster, and usually reach lower training error. This suggests selecting a generative classifier over discriminative classifiers in such setting.

For normal discriminant analysis and logistic regression family, a generative classifier usually approach its asymptotic error much faster too, but discriminative classifier often reach lower training error. This suggests selecting a  discriminative classifier over generative classifiers in such setting.

# Problem 12

Objective: comparing classifiers on synthetic data, application of
different classifiers. In this problem, you will compare different
classifiers using synthetic toy data sets.

Section 4.7.2 (Logistic regression) and 4.7.5 (NB) of ISLR_v2 (or ISLP)
contain helpful information for solving this problem.

Toy data sets

We have generated ten training data sets of different sizes
toy_train\_<n>.csv for $n \in {23, 25, ..., 212}$, and one test data set
toy_test.csv with 10000 points.

Each toy data set has a binary class variable $y \in {0,1}$and two
real-valued features $x_1$, $x_2 \in R$. The data are generated from the
"true" model as follows:

-   x_1 and x_2 are sampled from a normal distribution with zero mean
    and unit variance.

-   The probability of $y$ is given by:


$$
  p(y=1|x_1, x_2) = \sigma(-\frac{1}{2}-x_1 + 3 \times \frac{x_2}{2} + \frac{x_1 \times x_2}{3} )
$$


where $\sigma(t) = 1/(1 + e^{-t})$is the standard logistic function.

## Problem 12 Task a

### Question

Is the Naive Bayes (NB) assumption valid for the toy data set? Explain
why or why not.

### My Answer

NOT valid. The Naive Bayes (NB) assumption states that the features in a
data are conditionally independent given the class label. In the formula
of how y is generated, there is a interaction term
$\frac{x_1 \times x_2}{3}$, which means the effect of $x_1$ on $y$ is
modified by the effect of $x_2$ on $y$. That is, they are not
independent on each other.

## Problem 12 Task b

### Question

For each training set, train several classifiers that output
probabilities (described below), and then report their accuracy and
perplexity on the test set. Produce the following table (or make a plot)
for accuracy and for perplexity on the test set:

```{r}
p12.tab <- matrix(rep("?", 60), nrow = 10, ncol = 6)
colnames(p12.tab) <- c("n", "NB", "LR", "iLR", "Optimal Bayes", "Dummy")
p12.tab <- p12.tab |> data.frame()
for (i in 1:nrow(p12.tab)){
  p12.tab[i, "n"] <- 2^i
}
p12.tab |> kable()
```

where the columns correspond to: - Naive Bayes (NB) (e.g., naive Bayes
from the library e1071) - Logistic regression without an interaction
term (e.g., glm) - Logistic regression with an interaction term (e.g.,
glm) - Optimal Bayes classifier that uses the actual class conditional
probabilities (that you know in this case!) to compute $p(y|x_1,x2)$ for
a given $(x_1,x_2)$ - no probabilistic classifier can do better than
this.

`Dummy classifier` that does not depend on x. It always outputs the
probability $\hat{p} (y = 1 | x_1, x2)$ as the fraction of \$ y = 1\$ in
the training data. "Dummy" means that the classifier output does not
depend on the covariates. Including a dummy classifier in your
comparison is always a good idea! One way to get a dummy classifier here
is to train a logistic regression with only the intercept term.

### My answer

```{r}
library(e1071)
library(glmnet)

# Initialize the results table
p12.tab <- matrix(rep("?", 60), nrow = 10, ncol = 6)
colnames(p12.tab) <- c("n", "NB", "LR", "iLR", "Optimal Bayes", "Dummy")
p12.tab <- p12.tab |> data.frame()
for (i in 1:nrow(p12.tab)) {
  p12.tab[i, "n"] <- 2^(i + 2)
}
```

```{r}
#read data
test_data <- 
  read_csv(file.path(here(), "Exercise Sets-20231117", "E2", "data_E2", "toy_test.csv"))
```

```{r}
# Function to calculate the probability using the Optimal Bayes classifier
optimal_bayes_prob <- function(x1, x2) {
  t <- -1/2 - x1 + 3 * x2 / 2 + x1 * x2 / 3
  return(1 / (1 + exp(-t)))
}


```

```{r}
# Define a function to calculate accuracy and perplexity
calculate_metrics <- function(predicted_prob, actual) {
  # Calculate accuracy
  predicted_class <- ifelse(predicted_prob >= 0.5, 1, 0)
  accuracy <- mean(predicted_class == actual)

  # Calculate perplexity
  perplexity <- exp(-mean(actual * log(predicted_prob) + (1 - actual) * log(1 - predicted_prob)))

  return(c(accuracy, perplexity))
}

# Loop over each training set size
for (i in 1:nrow(p12.tab)) {
  n <- p12.tab[i, "n"]
  
  # Load the training data
  train_data <- read.csv(paste0(here(), "/Exercise Sets-20231117/E2/data_E2/toy_train_", n, ".csv"))

  # Naive Bayes
  NB_model <- naiveBayes(y ~ ., data = train_data)
  NB_prob <- predict(NB_model, test_data, type = "raw")[, 2]
  p12.tab[i, c("NB", "NB_perp")] <- calculate_metrics(NB_prob, test_data$y)

  # Logistic Regression without interaction term
  LR_model <- glm(y ~ ., data = train_data, family = binomial)
  LR_prob <- predict(LR_model, test_data, type = "response")
  p12.tab[i, c("LR", "LR_perp")] <- calculate_metrics(LR_prob, test_data$y)

  # Logistic Regression with interaction term
  iLR_model <- glm(y ~ . + x1 * x2, data = train_data, family = binomial)
  iLR_prob <- predict(iLR_model, test_data, type = "response")
  p12.tab[i, c("iLR", "iLR_perp")] <- calculate_metrics(iLR_prob, test_data$y)

  # Optimal Bayes
  test_data$optimal_bayes_prob <- with(test_data, optimal_bayes_prob(x1, x2))
  optimal_bayes_true_metrics <- calculate_metrics(test_data$optimal_bayes_prob, test_data$y)
  p12.tab[i, c("Optimal.Bayes", "Optimal.Bayes_perp")] <- optimal_bayes_true_metrics
  # Dummy classifier
  dummy_prob <- mean(train_data$y)
  dummy_metrics <- calculate_metrics(rep(dummy_prob, nrow(test_data)), test_data$y)
  p12.tab[i, c("Dummy", "Dummy_perp")] <- dummy_metrics
}

```

The table of accuracy is as follows:

```{r}
p12.tab |> 
  dplyr::select(
    !contains("_perp")
  ) |> 
  DT::datatable()
```

The table of perplexity is as follows:

```{r}
p12.tab |> 
  dplyr::select(
    n, contains("_perp")
  )|> 
  mutate_if(is.numeric, ~round(.,3)) |> 
  DT::datatable()
```

## Problem 12 Task c

### Question

Task c Report the logistic regression coeï¬€icients with interaction terms
for the largest training data set. How do they compare with the
coeï¬€icients of the actual model that generated the data? Discuss your
observations and what you can conclude.

-   Which of the models above are probabilistic, discriminative, and
    generative?

-   How do accuracy and perplexity (log-likelihood) compare?

-   Is there a relation to the insights from the previous problem?

-   Why does logistic regression with the interaction term perform so
    well for larger datasets?

-   Does your dummy classifier ever outperform other classifiers, or do
    different classifiers outperform the optimal Bayes classifier?

### My answer

This is the logistic regression coeï¬€icients with interaction terms for
the largest training data set:

```{r}
# Assuming the largest dataset is loaded as largest_train_data
# The model formula includes interaction between x1 and x2
largest_train_data <- read.csv(paste0(here(), "/Exercise Sets-20231117/E2/data_E2/toy_train_", "4096", ".csv"))
model <- glm(y ~ x1 + x2 + I(x1 * x2), data = largest_train_data, family = binomial())
coef(model)|> 
  as.data.frame() |> 
  rename(coefficients = 1) |> 
  mutate_if(is.numeric, ~round(.,3)) |> 
  DT::datatable()
```

In the exercise,

-   Logistic regression, optimal Bayes classifiers, and naive Bayesian
    probabilistic;

-   Logistic Regression (both with and without interaction terms) is a
    discriminative model.

-   Naive Bayes is a generative model.

Accuracy vs perplexity,

-   Accuracy measures the proportion of correct predictions.

    -   Higher accuracy, more correct classification.

-   Perplexity is a measure of how well the probability model predicts a
    sample.

    -   A less perplexed model is more confident and accurate in its
        predictions.

-   In our exercise, accuracy increases with the sample size increasing
    and hit the ceiling around hundreds.

-   In our exercise, perplexity decreases with the sample size
    increasing and hit the ceiling around hundreds.

-   In our exercise, lower perplexity model usually outperforms higher
    perplexity model in terms of accuracy, and higher accuracy model
    usually outperforms lower perplexity model in terms of perplexity.

Why logistic regression with interaction term works well?

-   Because it captures the interactive relationship between $x_1$ and
    $x_2$ in the model, which is pre-defined in the true model in our
    exercise.

-   With increasing sample size, model variance is decreasing. Such
    pattern of interaction is more pronounced.

Does your dummy classifier ever outperform others?

-   No. Generally it works worse than other classifiers.

-   Exceptions are when the sample sizes are extremely small/

Do different classifiers outperform the optimal Bayes classifier?

-   No. Optimal Bayes classifier uses the true model and it generally
    outperforms other classifiers.

-   Exceptions are one iLR classifier with large sample size by very
    small margin, which might be due to over-fitting. With even larger
    sample, this edge disappears.

# Problem 13

[6 points] Objectives: basic principles of decision trees

```{r}
knitr::include_graphics(file.path(here(), "data", "p13.png")) 
```

In this task, you will simulate a decision tree algorithm by hand using
the toy data shown in the figure. Read Section 8.1 of ISLR_v2. Use the
Gini index of Equation (8.6) as an impurity measure.

(You do not need to worry about overfitting here: the resulting
classification tree should have enough splits to fit the training data
without error. Don't worry if your results are not optimal or
super-accurate, as long as they are "in the ballpark".)

## Problem 13 Task a

```{r}
a <- 1-((15/23)^2+(1-15/23)^2)
b <- 1-((8/15)^2+(1-8/15)^2)
c <- 1-((8/12)^2+(1-8/12)^2)
d <- 1-((4/5)^2+(1-4/5)^2)
```

```{r}
knitr::include_graphics(file.path(here(), "data", "tree.png")) 
```

# Problem 14

Learning objectives: basics of the k-NN method.

In this task, you will apply the k-nearest neighbour (k-NN) classifier
by hand on a toy data set. You should be able to do this with pen and
paper. We will use the training dataset $D = {(x_i, c_i)}_{i=1}^{14}$ ,
shown below, where ð‘¥ âˆˆ R are the co-variates and $c_i \in \{-1, +1\}$
are the classes.

```{r}
p14.matrix <- 
  matrix(
    c(0,2,3,5,6,8,9,12,13,15,16,18,19,21, "+1","+1","+1","-1","+1","+1","+1","-1","-1","-1","+1","-1","-1","-1"),
    nrow = 2, ncol = 14, byrow =  T
  )
colnames(p14.matrix) <- 1:14
rownames(p14.matrix) <- c("xi", "ci")

p14.matrix |> kable()
```

## Problem 14 Task a

### Question

Where are the classification boundaries for the 1-NN and 3-NN
classifiers? What are the respective classification errors on the
training dataset?

### My answer

For 1-NN classifier, the classification boundaries are **4, 5.5, 10.5, 15.5 and 17**. The classification error is **0**.

For 3-NN classifier, the classification boundaries are **10.5**. The classification error is **0.143**.

## Problem 14 Task b

### Question

How does the choice of k in k-NN affect the classification boundary (not
in the above example but in general)? Give examples of the behaviour for
extreme choices (very small or large $k$).

### My answer

The choice of K has a drastic effect on the KNN classifier obtained.
When K is extremely small, the decision boundary is overly flexible and
finds patterns in the data that don't correspond to the Bayes decision
boundary. This corresponds to a classifier that has low bias but very
high variance. As K grows, the method becomes less flexible and produces
a decision boundary that is close to linear. This corresponds to a
low-variance but high-bias classifier. For example, if we select
$k = 1$, the classifier over-fits the data, leaving a training error of
0 but high testing error; On the other hand, if we select a very large
k, say 100, the classifier will become less flexible with very big
variance, producing high testing error.

# Problem 15

Topic: SVM

In this problem, you will study the support vector machine (SVM)
classifier on the toy data set shown below.

```{r}
p15.matrix <- 
  matrix(
    c(-1.5, 0, 1,0, 0.9, 1, 0.2, -1.8, 1, 2, 0, 1,4, 1, -1, 4, -1, -1, 6, -0.5, -1,  7, 1, -1),
    nrow = 8, ncol = 3, byrow = T
  )

colnames(p15.matrix) <- c("x1", "x2", "class")

rownames(p15.matrix) <- LETTERS[1:8]
```

Find a separating hyperplane with the largest margin on the data set
above. write down the equation for this hyperplane and report the margin
size.

Which of the points(A-H) are support vectors?

Hint: You can answer without mathematical proofs. You can do it simply
by geometric intuition.

```{r}
knitr::include_graphics(file.path(here(), "data", "p15.png")) 
```

My intuition is points D, E and F are support vectors. The points are
plotted as below, with hyperlane plotted as a thick line and points on
dashed lines are support vectors.

```{r}
# Given dataset matrix
p15.matrix <- 
  matrix(
    c(-1.5, 0, 1, 0, 0.9, 1, 0.2, -1.8, 1, 2, 0, 1, 4, 1, -1, 4, -1, -1, 6, -0.5, -1, 7, 1, -1),
    nrow = 8, ncol = 3, byrow = T
  )

# Convert matrix to data frame for plotting
p15.df <- as.data.frame(p15.matrix)
colnames(p15.df) <- c("x1", "x2", "class")
plot(p15.df$x1, p15.df$x2, col=factor(p15.df$class), pch=19, xlab="x1", ylab="x2")

abline(v=3, lwd = 3) 

abline(v=2, lty = 2) 
abline(v=4, lty = 2) 

arrows(2,0,3,0, length = 0.1)
arrows(3,0,2,0, length = 0.1)

arrows(4,1,3,1, length = 0.1)
arrows(3,1,4,1, length = 0.1)

arrows(4,-1,3,-1, length = 0.1)
arrows(3,-1,4,-1, length = 0.1)

text(2.5, 0.1, "Margin")
text(3.5, 0.8, "Margin")
text(3.5, -1.2, "Margin")
```

The Hyperplane Equations for points D, E and F are

-   D: $a \times 2.0 + b \times 0 + c = 0$

-   E: $a \times 4.0 + b \times 1 + c = 0$

-   F: $a \times 4.0 + b \times -1 + c = 0$

The margins are 1, 1 and 1, respectively. The sum of margins are 3. 

# Problem 16

## Problem 16 Task a

### Question

Write a learning diary of the topics of lectures 5-8 and this exercise
set.

Instructions

Guiding questions: What did I learn? What did I not understand? Was
there something relevant to other studies or (future) work? Your reply
should be 1-3 paragraphs of text. You can also give feedback on the
course.

During these sessions, I learnt the logic behind a number of
classifiers, including logistic regression, LDA, naive Bayesian, SVM,
decision trees, bagging, random forests and ensemble methods. I followed
the math behind some of these techniques. I also reviewed a number of
distance metrics.

I didn't manage to follow some of the mathematical proofs, especially
support vector machine. Some of the distance metrics mentioned in the
class sounds very new to me. I am still trying to get my head around
them, especially which metrics fits which scenario.

In the sessions I found some math blind spots (such as optimization) and
I will make them up by joining more basic level courses, after this
course.

## Problem 16 Task b

### Question

Give an estimate of the hours used in solving the problems in this
exercise set.

### My answer

12 hours.
