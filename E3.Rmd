---
title: "E3"
output: html_document
date: "2023-12-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, warning = F, message = F, echo = FALSE)
```

# Problem 17 *[9 points]*

Objectives: k-means loss and Lloyd’s algorithm

In this problem, you will study the (naive) k-means algorithm (Lloyd’s algorithm). You should be able to solve this problem with a pen and paper. See Section 12.4.1 of ISLR_v2.


## Problem 17: Task a

### Question

Answer the following:

1. For what kinds of tasks can we use the k-means algorithm? 2. What are the algorithm’s inputs and outputs?

3. How should you interpret the results?

### Answer

It's for clustering task. It helps identify hidden patterns in the data, especially when we do not have any existing knowledge about the true clusters.

## Problem 17: Task b

### Question

Define the objective (or cost) function the k-means algorithm tries to minimise.

What can you say about the objective function’s value during the algorithm iteration?


### Answer

The k-means algorithm aims to minimize an objective function known as the within-cluster sum of squares (WCSS) or inertia. This objective function quantifies the variance within each cluster and is defined as the sum of squared distances between each data point and the centroid of its assigned cluster. Mathematically, the WCSS objective function can be expressed as follows:

## Problem 17: Task c

### Question

Consider the following toy data set:

```{r}
p17.table <- 
  matrix(
    c(0,1,4,5,5,
      1,2,5,3,4),
    byrow = T,
    nrow = 2,
    ncol = 5
  )
colnames(p17.table) <- 1:5
rownames(p17.table) <- c("x","y")
p17.table |> 
  DT::datatable()
```

```{r fig.height=3.5, fig.width=3.5}
p17.table |> t() |> plot(cex =0.1)
text(p17.table |> t() |> data.frame()|> dplyr::pull(x), 
     p17.table |> t() |> data.frame()|> dplyr::pull(y),
     labels = p17.table |> t() |> row.names())
```

Sketch a run of the (naive) k-means algorithm using $K = 2$ and initial prototype (mean) vectors $\mu_1$ = (0, 2) and $\mu_2$ = (2, 0). Write down the calculation procedure at each iteration and report the cluster memberships, the prototype vectors, and the value of the objective function at each iteration.

### Answer

Calculate the Euclidean distance from each point to each prototype and assign each point to the closest prototype.

```{r}
euclidean <- function(a, b) sqrt(sum((a - b)^2))
WCSS <- function (a,b)sum((a-b)^2)
```

Euclidean distance of point 1 to $\mu1$ and $\mu2$. Results show it's closer to $\mu1$.

```{r}
point1_to_mu1 <- euclidean(c(0,2), p17.table[,1])
point1_to_mu2 <- euclidean(c(2,0), p17.table[,1])
p1.assign <- which.min(c(point1_to_mu1, point1_to_mu2))
```
Euclidean distance of point 2 to $\mu1$ and $\mu2$. Results show it's closer to $\mu1$.

```{r}
point2_to_mu1 <- euclidean(c(0,2), p17.table[,2])
point2_to_mu2 <- euclidean(c(2,0), p17.table[,2])
p2.assign <- which.min(c(point2_to_mu1, point2_to_mu2))
```

Euclidean distance of point 3 to $\mu1$ and $\mu2$. Results show it's closer to $\mu2$.

```{r}
point3_to_mu1 <- euclidean(c(0,2), p17.table[,3])
point3_to_mu2 <- euclidean(c(2,0), p17.table[,3])
p3.assign <- which.min(c(point3_to_mu1, point3_to_mu2))
```

Euclidean distance of point 4 to $\mu1$ and $\mu2$. Results show it's closer to $\mu2$.

```{r}
point4_to_mu1 <- euclidean(c(0,2), p17.table[,4])
point4_to_mu2 <- euclidean(c(2,0), p17.table[,4])
p4.assign <- which.min(c(point4_to_mu1, point4_to_mu2))
```

Euclidean distance of point 5 to $\mu1$ and $\mu2$. Results show it's closer to $\mu5$.

```{r}
point5_to_mu1 <- euclidean(c(0,2), p17.table[,5])
point5_to_mu2 <- euclidean(c(2,0), p17.table[,5])
p5.assign <- which.min(c(point5_to_mu1, point5_to_mu2))
```

To summarize iteration 1

```{r}
library(tidyverse)
iteration1.table <- 
  data.frame(
  point = 1:5,
  x = p17.table[1,],
  y = p17.table[2,],
  'distance to mu1' = c(point1_to_mu1, point2_to_mu1, point3_to_mu1, point4_to_mu1, point5_to_mu1),
  'distance to mu2' = c(point1_to_mu2, point2_to_mu2, point3_to_mu2, point4_to_mu2, point5_to_mu2),
  'cluster assignment' = c(p1.assign, p2.assign, p3.assign, p4.assign, p5.assign)
) 
iteration1.table |> 
  dplyr::mutate_if(is.numeric, ~round(.x,2)) |> 
  DT::datatable()
```

Then I recalculate prototypes:

```{r}
iteration2.mu1.x <- 
  iteration1.table |> 
  filter(cluster.assignment == 1) |> 
  pull(x) |> 
  mean()
iteration2.mu1.y <- 
  iteration1.table |> 
  filter(cluster.assignment == 1) |> 
  pull(y) |> 
  mean()
iteration2.mu2.x <- 
  iteration1.table |> 
  filter(cluster.assignment == 2) |> 
  pull(x) |> 
  mean()
iteration2.mu2.y <- 
  iteration1.table |> 
  filter(cluster.assignment == 2) |> 
  pull(x) |> 
  mean()

iteration2.mu1 <- c(iteration2.mu1.x, iteration2.mu1.y)
iteration2.mu2 <- c(iteration2.mu2.x, iteration2.mu2.y)
```

```{r}
iteration1.c1.xy <- iteration1.table[which(iteration1.table$cluster.assignment == 1), c("x","y")]
iteration1.c2.xy <- iteration1.table[which(iteration1.table$cluster.assignment == 2), c("x","y")]
WCSS.c1 <- 
  sum(
    apply(
      iteration1.c1.xy, 
      1, 
      function(x)WCSS(x, iteration2.mu1)
      )
    )

WCSS.c2 <- 
  sum(
    apply(
      iteration1.c2.xy, 
      1, 
      function(x)WCSS(x, iteration2.mu2)
      )
    )

WCSS.c1+WCSS.c2
```

The objective function WCSS is `r WCSS.c1+WCSS.c2`

I did the above steps iteratively. 

```{r}
distance_table <- data.frame(
    point = 1:5,
    x = p17.table[1,],
    y = p17.table[2,],
    'distance to mu1' = NA,
    'distance to mu2' = NA,
    'cluster assignment' = NA
  )

k.iterate <- function(c1_centroid, c2_centroid){
  for(i in 1:5){
    p1_to_mu1_dist <- euclidean(p17.table[,i], c1_centroid)
    p1_to_mu2_dist <- euclidean(p17.table[,i], c2_centroid)
    assignment <- which.min(c(p1_to_mu1_dist, p1_to_mu2_dist))
    distance_table[i,4] <- p1_to_mu1_dist
    distance_table[i,5] <- p1_to_mu2_dist
    distance_table[i,6] <- assignment
  2
  print(distance_table)
  c1.new.x <- distance_table  |>  filter(assignment == 1) |> pull(x) |> mean()
  c1.new.y <- distance_table  |>  filter(assignment == 1) |> pull(y) |> mean()
  c2.new.x <- distance_table  |>  filter(assignment == 2) |> pull(x) |> mean()
  c2.new.y <- distance_table  |>  filter(assignment == 2) |> pull(y) |> mean()
  new.c1.centroid <- c(c1.new.x, c1.new.y)
  new.c2.centroid <- c(c2.new.x, c1.new.y)
  
}

k.iterate(iteration2.mu1, iteration2.mu2)
```


# Problem 18

## Problem 18: Task a

# Problem 19 *[9 points]*

Objectives: practical application of k-means and hierarchical clustering
For this problem, you will apply clustering on a subset of the project data (mol.csv). See Section 12.5.3 of ISLR_v2.
When clustering the data, omit the columns parentspecies and pSat_Pa, and scale the other variables to zero mean and unit variance unless otherwise instructed.
You should use library functions, such as kmeans in R and sklearn.cluster.KMeans in Python. Note: In R, kmeans uses the Hartigan-Wong algorithm by default, so set algorithm="Lloyd" to use Lloyd’s algorithm. Python’s KMeans uses kmeans++ as the default initialization, so set init="random" to use random initialisation in Task b.

## Problem 19: Task a 

### Question

Plot (or report in a table) the k-means loss as a function of the number of clusters from 1 to 20 for scaled and non-scaled data.
Should you scale the columns? How does scaling the columns affect the result?

### Answer

## Problem 19: Task b

### Question

The initialisation of k-means can affect the solution.

Cluster the data with k-means using 𝑘 = 5 with 1000 different random initialisations, and then answer the following:

• What are the minimum and maximum k-means losses for your 1000 random initialisations?

• How many initialisations would you expect to have to obtain one reasonably good loss for this data set and number of clusters? A suﬀiciently good loss here is a solution with a loss within 1% of the best loss out of your 1000 losses.

• How do we deal with the effect of initialization when using k-means in practice?

Make a histogram of the losses.

### My answer

## Problem 19: Task c

### Question

Task c
(i) Cluster the data with agglomerative hierarchical clustering using single, complete, and Ward linkage. Produce their dendrograms side-by-side.

(ii) Find and report at least one interesting feature or reproduce some properties of hierarchical clustering discussed in the class or the textbook. For example, you can show differences between the linkage functions by comparing cluster sizes in different flat clusterings (cutree in R, scipy.cluster.hierarchy.cut_tree in Python).

### answer

# Problem 20 *[9 points]*

In this problem, you will apply Principal Component Analysis (PCA) to mol.csv. See Sect. 12.2.4 of ISLR_v2.
When computing PCA on the data, omit the columns parentspecies and pSat_Pa, and scale the other variables to zero mean and unit variance unless otherwise instructed.

## Problem 20: Task a

### Question

Compute and show a PCA projection of the data into two dimensions.
Indicate the parent species (column parentspecies) by the point’s colour and shape. Remember to include a legend that indicates which colour/shape corresponds to which class.
What does the plot imply about the relationship between parentspecies and the other variables?

### My answer

```{r}
# Load necessary libraries
library(tidyverse)
library(FactoMineR)
# Read the data
file.name <- "mol.csv"

mol <- read_csv(file.path(here::here(), "exercise_sets", "E3", file.name))
# scale all cols but parentspecies and pSat_Pa
mol_unscaled<- mol  |>  
  mutate(
    across(
      !c(
        parentspecies, 
        pSat_Pa
        ),
      as.numeric
      )
  )

mol_scaled <- mol  |>  
  mutate(
    across(
      !c(
        parentspecies, 
        pSat_Pa
        ),
      ~scale(as.numeric(.))
      )
  )
```

```{r}
selected.cols.pca <- 
  mol_scaled |> 
  dplyr::select(
    -parentspecies, 
    -pSat_Pa
    ) |> 
  names() 
   
# Do PCA
pca_output <- 
  PCA(
    mol_scaled |> select(one_of(selected.cols.pca)), 
    graph = FALSE
    )

#extract coordinates
pca_coord <- pca_output$ind$coord |> data.frame()

# append parentspecies col

pca_coord$parentspecies <- factor(mol$parentspecies)

# Plotting the PCA
## n of levels for parentspecies
n_category <- length(unique(pca_coord$parentspecies))
##color scheme
library(wesanderson)
## plot
pca_coord |> 
  ggplot(
    aes(
      x = Dim.1, 
      y = Dim.2, 
      color = parentspecies
      )
  ) +
  geom_point(shape = 21, color = "black") +
  geom_point(alpha = 0.5, shape = 10) +
  scale_color_manual(
    #values = wes_palette(n = 3, name = "FantasticFox1"),
    values = c("red", "blue", "yellow"),
    name = "Parent Species"
    ) +
  labs(title = "PCA Projection with Parent Species",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme(legend.title = element_text(size = 10, family = "mono"),
        legend.text = element_text(size = 8, family = "mono"),
        legend.key = element_rect(fill = "white"),
        plot.title = element_text(family = "mono"),
        axis.text = element_text(family = "mono"),
        axis.title = element_text(family = "mono"),
        panel.background = element_rect(fill = "white", color = "black"),
        legend.position = "bottom"
        )+
  coord_fixed()

```

Although there's considerable overlap between points of different Parent Species, noticeable patterns exist in the distribution of the points along the principal component axes: 

  -Species 'decane' and 'toluene' forms different clusters, suggesting they have unique molecular signatures that are captured by
  the principal components. 
  
  -Species `decane` and `apin` are clustered less distinctively than they are to species `toluene`, indicating share certain
  molecular characteristics that are significantly
  different from `toluene` in terms of features captured by PC1 and PC2.
  
  -Species `toluene` distributed across both PCs evenly,indicating its molecular properties with highest variance can be equally
  and nicely represented by PC1 and PC2 nicely. 
  
  -Whereas, species `decane` and `apin` distribute mostly across the right end of PC1 axis and lower end of PC2 axis, suggesting
   most varying features are captured by a portion of PC1 and PC2 and these portions play a role in differentiating them from
   `toluene`


## Problem 20: Task b

### Question

Plot (or report in a table) the proportion of variance explained (PVE), and the cumulative PVE for the principal components for scaled and un-scaled data.

Why does it seem that fewer components explain a large proportion of the variance for un-scaled data compared to the scaled data?

```{r}
#scaled pca results
pca_output_scaled <- 
  PCA(
    mol_scaled |> select(one_of(selected.cols.pca)), 
    graph = FALSE
    )
#un-scaled pca results
pca_output_unscaled <- 
  PCA(
    mol_unscaled |> select(one_of(selected.cols.pca)), 
    scale.unit = F,
    graph = FALSE
    )

#libarary to extract variance
library(factoextra)

#get variances
variance_scaled <- get_eigenvalue(pca_output_scaled) |> data.frame()
variance_scaled$Dimension <-1:nrow(variance_scaled)
variance_scaled$label <- "scaled"


variance_unscaled <- get_eigenvalue(pca_output_unscaled) |> data.frame()
variance_unscaled$Dimension <- 1:nrow(variance_unscaled)
variance_unscaled$label <- "unscaled"

variance_table <- rbind(variance_scaled, variance_unscaled)
```

```{r fig.height=3, fig.width= 6}
#plot
facet <- c("Scaled", "Unscaled")
names(facet) <- c("scaled", "unscaled")

variance_table |> 
  ggplot(aes(x = Dimension, y = variance.percent))+
  geom_bar(stat = "identity", fill = "wheat", color = "black")+
  geom_line(size = 0.5)+
  geom_text(aes(label = round(variance.percent,2)), size = 1.8, vjust = -0.3)+
  facet_wrap(~label, ncol = 1, scales = "free", labeller = labeller(label = facet))+
  labs(title = "Scaled and unscaled variance explained by numbers of principal component", y = "Variance Explained")+
  theme(legend.title = element_text(size = 10, family = "mono"),
        legend.text = element_text(size = 8, family = "mono"),
        legend.key = element_rect(fill = "white"),
        plot.title = element_text(family = "mono", size  = 10),
        axis.text = element_text(family = "mono"),
        axis.title = element_text(family = "mono"),
        panel.background = element_rect(fill = "white", color = "black"),
        legend.position = "bottom",
        strip.background = element_rect(color = "black", fill = "lightblue")
        )
```

With un-standardized data set, PCA produced results showing PC1 explains 93% of the variability of the data set; other components’ contribution is less the 7%, in totality . With standardized data set, PC1 and PC2 together explains 35% of the variability of the data set, with the amount of variability explained falling gradually for the components following. Base on finding above, it is not hard to draw the conclusion that PCA using standardized data set produces results better for analysis. Possible explanation for this is the different scales of the variables make comparison between pairs of features difficult. PCA is calculated based on co-variance. Unlike correlation, which is dimensionless, covariance is in units obtained by multiplying the units of the two variables. When data set is not scaled, this makes each variable not easily comparable with others (since they all have their own value ranges). Further, each variable loads almost exclusively on one components because they can hardly find another variable with comparable value range. 

### My answer

## Problem 20: Task c

### Questions

In this task, you will use ordinary least-squares linear regression to predict log10(pSat_Pa) using all variables except parentspecies. Use the random split of mol.csv in files mol_train.csv and mol_validation.csv we have provided.

(i) Train the model on the training set mol_train.csv (without dimensionality reduction) and report the RMSE on the validation set mol_validation.csv.

(ii) Repeat (i) after reducing the dimensionality with PCA for all dimensions ${0, 1, ... , p}$, where 𝑝 is the number of covariates, and report the RMSE values in a table or a plot. How does your model’s performance vary with the (reduced) dimensionality? Is there an “optimal” dimensionality which gives you the best performance on the validation set?

Hint: the dimensionality zero means here that your covariates should consist only of the intercept term (i.e., you have no PCA components).

(iii) What is the smallest dimensionality that gives you a validation set RMSE that is at most 1% larger than the RMSE on dimensionality with the smallest RMSE? Argue why this dimensionality could be a better choice to learn a model than the “optimal” dimensionality you found in subtask (ii) above.

Hint: You can just mention here that the loss you got in subtask (ii) is already the smallest dimensionality within 1% of the best validation loss if this is your result.

Tip: Notice that you can apply PCA on the combined training and validation sets to utilise the structure of the validation set even if you don’t know the class labels; this is a simple form of semi-supervised learning.

### My answer

```{r}
# Read the data
file.name.train <- "mol_train.csv"
file.name.validation <- "mol_validation.csv"

mol_train <- read_csv(file.path(here::here(), "exercise_sets", "E3", file.name.train))
mol_validation <- read_csv(file.path(here::here(), "exercise_sets", "E3", file.name.validation))
```

```{r}
# Prepare the data
x_train <- mol_train  |>  select(-parentspecies, -pSat_Pa)
y_train <- log10(mol_train$pSat_Pa)

x_validation <- mol_validation |>  select(-parentspecies, -pSat_Pa)
y_validation <- log10(mol_validation$pSat_Pa)

# Linear regression without dimensionality reduction
fit.initial <- lm(y_train ~ ., data = x_train)
y_initial_prediction <- predict(fit.initial, newdata = x_validation)
rmse_initial <- sqrt(mean((y_validation - y_initial_prediction)^2))
```

```{r}
RMSE_table <- data.frame(n_dimension = 0:ncol(x_train), RMSE = NA)
for (p in 0:ncol(x_train)){
  if (p == 0){
   fit <- lm(y_train ~ 1)
   y_prediction <- predict(fit, newdata = x_validation)
   rmse <- sqrt(mean(y_validation - y_prediction)^2)
   RMSE_table[1,2] <- rmse
  } else{
  pca_fit <- PCA(x_train, ncp = p, graph = FALSE)
  pca_scores <- pca_fit$ind$coord |> data.frame()
  fit <- lm(y_train ~ ., data = pca_scores)
  x_validation_pca <- predict(pca_fit, newdata = x_validation)
  x_validation_pca_scores <- x_validation_pca$coord |> data.frame()
  y_prediction <- predict(fit, newdata = x_validation_pca_scores)
  rmse <- sqrt(mean(y_validation - y_prediction)^2)
  RMSE_table[p+1,2] <- rmse 
  }
}

RMSE_table |> 
  ggplot(aes(x = n_dimension, y = RMSE)) +
  geom_line()+
  labs(title = "Number of dimension vs RMSE", x = "Number of dimensions")+
  theme(legend.title = element_text(size = 10, family = "mono"),
        legend.text = element_text(size = 8, family = "mono"),
        legend.key = element_rect(fill = "white"),
        plot.title = element_text(family = "mono", size  = 10),
        axis.text = element_text(family = "mono"),
        axis.title = element_text(family = "mono"),
        panel.background = element_rect(fill = "white", color = "black"),
        legend.position = "bottom",
        strip.background = element_rect(color = "black", fill = "lightblue")
        )+
  geom_point(
    aes(
      x = RMSE_table$n_dimension[(RMSE_table$RMSE |> which.min())],
      y = min(RMSE)
      ), 
    color = "red", 
    size = 8,
    shape = 21
    )

```

RMSE_table

```{r}
min_RMSE <- RMSE_table$RMSE |> min()
RMSE_table |> 
  filter(
    RMSE < min_RMSE*1.1
  ) |> 
  filter(
    n_dimension == min(n_dimension)
  )
```

# Problem 21 *[2 points]*

## Problem 21: Task a

### Question

Write a learning diary of the topics of lectures 9–10 and this exercise set.

Guiding questions: What did I learn? What did I not understand? Was there something relevant to other studies or (future) work? Your reply should be 1-3 paragraphs of text. You can also give feedback on the course.

### Answer

The smallest dimensionality that gives me a validation set RMSE that is at most 1% larger than the RMSE on dimensionality with the smallest RMSE is 3.It can be better choice than the best accuracy oen for several reasons:

  - It balances between over-fitting and complexity. 
  
    The optimal number of dimension is 3, whereas the one with smallest RMSE has 15 dimensions. With such a large number of dimensions, it stands much higher risk of over-fitting.
    
  - Fewer-dimension model is less computational expensive.
  
  - Fewer-dimension model is robust to variations in the data.
  
  - Fewer-dimension model is easier to interpret. In special scenario like factor analysis, researchers may come up with the real-world correspondence to latent feature behind the dimesion. It's easier to do so if the number of dimensionality is limited.
  
  
  
  






